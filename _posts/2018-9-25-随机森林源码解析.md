---
layout: post
title: "Spark中随机森林组件"
author: "khashwung"
categories: 算法
tags: [documentation,sample]
---

## case class LabeledPoint

- 记录一个sample，或者instance。包含一个用于表示`label`的`Double`和表示`feature`的`Vector`



### trait Impurity

- 在`Strategy`类中指定，用于计算信息增益

- 针对分类和回归分别定义了一套计算接口

  ```scala
  /**
     * :: DeveloperApi ::
     * information calculation for multiclass classification
     * @param counts Array[Double] with counts for each label
     * @param totalCount sum of counts for all labels
     * @return information value, or 0 if totalCount = 0
     */
  def calculate(counts: Array[Double], totalCount: Double): Double
  
  /**
     * :: DeveloperApi ::
     * information calculation for regression
     * @param count number of instances
     * @param sum sum of labels
     * @param sumSquares summation of squares of the labels
     * @return information value, or 0 if count = 0
     */
  def calculate(count: Double, sum: Double, sumSquares: Double): Double
  ```

  

- 三个实现 ：`Gini`，`Entropy`，`Variance` 



## class Strategy

- 树模型配置类
  - `algo`：指定训练目标，是分类还是回归。取值为`object Algo extends Enumeration`的两个枚举元素`Classification`和`Regression`
  - `impurity`：信息增益的计算方式。分类为`Gini`或`Entropy`，回归为`Variance`
  - `maxDepth`：树的最大深度
  - `numClasses`：分类任务时的类个数，回归忽略该参数
  - `maxBins`：连续特征会离散成bins的形式。该参数指定在每个节点要分裂时，针对连续变量，要离散成多少bins。默认32
  - `quantileCalculationStrategy`：分位数的计算策略，在节点分裂选择最优切割点时会用到
  - `categoricalFeaturesInfo`：对于类别变量，该参数是一个存`(feature i -> k categories)`的map，指定每个特征有多少个分类取值
  - `minInstancesPerNode`：如果某个节点分裂后，产生的节点里包含的样本数量小于该参数，该节点将不分裂，默认为1
  - `minInfoGain`：如果某个节点分裂后，信息增益小于该参数，则该次分类将不会进行
  - `subsamplingRate`：对样本进行降采样

- 注意：`numTrees`参数不在这个配置类里，因为这是单棵树模型的配置类，而非随机森林的，这样可树模型配置类可以被其它ensemble模型重用

  

## class RandomForest

- 随机森林模型类，包含以下配置

  - `strategy`：单棵树模型的配置类

  - `numTrees`：ensemble模型树的个数

  - `featureSubsetStrategy`：三种指定方式，默认`auto`

    ```scala
    // org.apache.spark.mllib.tree.RandomForest
    require(RandomForest.supportedFeatureSubsetStrategies.contains(featureSubsetStrategy)
            || Try(featureSubsetStrategy.toInt).filter(_ > 0).isSuccess
            || Try(featureSubsetStrategy.toDouble).filter(_ > 0).filter(_ <= 1.0).isSuccess,
            s"RandomForest given invalid featureSubsetStrategy: $featureSubsetStrategy." +
            s" Supported values: ${NewRFParams.supportedFeatureSubsetStrategies.mkString(", ")}," +
            s" (0.0-1.0], [1-n].")
    ```

    

    - `(0.0-1.0)`区间的数
    - `[1-n]`区间的数
    - `auto`，`all`，`onethird`，`sqrt`或`log2`

- 执行逻辑

  调用

  ```scala
  val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
                                           numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)
  ```

  进入

  ```scala
  def trainClassifier(
      input: RDD[LabeledPoint],
      strategy: Strategy,
      numTrees: Int,
      featureSubsetStrategy: String,
      seed: Int): RandomForestModel = {
      require(strategy.algo == Classification,s"RandomForest.trainClassifier given Strategy with invalid algo: ${strategy.algo}")
      val rf = new RandomForest(strategy, numTrees, featureSubsetStrategy, seed)
      rf.run(input)
  }
  ```

  然后

  ```scala
  def run(input: RDD[LabeledPoint]): RandomForestModel = {
      val trees: Array[NewDTModel] = NewRandomForest.run(input.map(_.asML), strategy, numTrees,featureSubsetStrategy, seed.toLong, None)
      new RandomForestModel(strategy.algo, trees.map(_.toOld))
  }
  ```

  `NewRandomForest`，`NewLabeledPoint`都是和mllib对应的ml库的组件，相同内容，不同库里的不同叫法。

  

## trait Split

- 一个接口，用于指定在特定tree node处，测试是否选择左子树还是右子树

- 接口定义如下：

  ```scala
  private[ml] def shouldGoLeft(features: Vector): Boolean
  private[tree] def shouldGoLeft(binnedFeature: Int, splits: Array[Split]): Boolean
  ```

- 两个实现类：`CategoricalSplit`和`ContinuousSplit`，判断是否进入左树的依据如下：

  - 对于`CategoricalSplit`，存以下变量

    ```scala
    class CategoricalSplit private[ml] (
        override val featureIndex: Int,
        _leftCategories: Array[Double],
        @Since("2.0.0") val numCategories: Int)
    ```

    如果`featureIndex`对应的特征值在`_leftCategories`中，则进入左树，否则进入右树

  - 对于`ContinuousSplit`，存以下变量

    ```scala
    class ContinuousSplit private[ml] (override val featureIndex: Int, val threshold: Double)
    ```

    如果`featureIndex`对应的特征值小于等于`threshold`，则进入左子树

- 注意：每个**特征**的每个**切割点**都有一个Split





## object ml.tree.impl.RandomForest

- 真正的随机森林实现类

- 训练核心代码如下

  ```scala
    def run(
        input: RDD[LabeledPoint],
        strategy: OldStrategy,
        numTrees: Int,
        featureSubsetStrategy: String,
        seed: Long,
        instr: Option[Instrumentation[_]],
        parentUID: Option[String] = None): Array[DecisionTreeModel] = {
  
      val retaggedInput = input.retag(classOf[LabeledPoint])
      val metadata = DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)
      val splits = findSplits(retaggedInput, metadata, seed)
  
  
      // Bin feature values (TreePoint representation).
      // Cache input RDD for speedup during multiple passes.
      val treeInput = TreePoint.convertToTreeRDD(retaggedInput, splits, metadata)
  
      val withReplacement = numTrees > 1
  
      val baggedInput = BaggedPoint
        .convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed)
        .persist(StorageLevel.MEMORY_AND_DISK)
  
      // depth of the decision tree
      val maxDepth = strategy.maxDepth
      require(maxDepth <= 30,
        s"DecisionTree currently only supports maxDepth <= 30, but was given maxDepth = $maxDepth.")
  
      // Max memory usage for aggregates
      // TODO: Calculate memory usage more precisely.
      val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024L
      logDebug("max memory usage for aggregates = " + maxMemoryUsage + " bytes.")
  
      /*
       * The main idea here is to perform group-wise training of the decision tree nodes thus
       * reducing the passes over the data from (# nodes) to (# nodes / maxNumberOfNodesPerGroup).
       * Each data sample is handled by a particular node (or it reaches a leaf and is not used
       * in lower levels).
       */
  
      // Create an RDD of node Id cache.
      // At first, all the rows belong to the root nodes (node Id == 1).
      val nodeIdCache = if (strategy.useNodeIdCache) {
        Some(NodeIdCache.init(
          data = baggedInput,
          numTrees = numTrees,
          checkpointInterval = strategy.checkpointInterval,
          initVal = 1))
      } else {
        None
      }
  
      /*
        Stack of nodes to train: (treeIndex, node)
        The reason this is a stack is that we train many trees at once, but we want to focus on
        completing trees, rather than training all simultaneously.  If we are splitting nodes from
        1 tree, then the new nodes to split will be put at the top of this stack, so we will continue
        training the same tree in the next iteration.  This focus allows us to send fewer trees to
        workers on each iteration; see topNodesForGroup below.
       */
      val nodeStack = new mutable.Stack[(Int, LearningNode)]
  
      val rng = new Random()
      rng.setSeed(seed)
  
      // Allocate and queue root nodes.
      val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))
      Range(0, numTrees).foreach(treeIndex => nodeStack.push((treeIndex, topNodes(treeIndex))))
  
      timer.stop("init")
  
      while (nodeStack.nonEmpty) {
        // Collect some nodes to split, and choose features for each node (if subsampling).
        // Each group of nodes may come from one or multiple trees, and at multiple levels.
        val (nodesForGroup, treeToNodeToIndexInfo) =
          RandomForest.selectNodesToSplit(nodeStack, maxMemoryUsage, metadata, rng)
        // Sanity check (should never occur):
        assert(nodesForGroup.nonEmpty,
          s"RandomForest selected empty nodesForGroup.  Error for unknown reason.")
  
        // Only send trees to worker if they contain nodes being split this iteration.
        val topNodesForGroup: Map[Int, LearningNode] =
          nodesForGroup.keys.map(treeIdx => treeIdx -> topNodes(treeIdx)).toMap
  
        // Choose node splits, and enqueue new nodes as needed.
        timer.start("findBestSplits")
        RandomForest.findBestSplits(baggedInput, metadata, topNodesForGroup, nodesForGroup,
          treeToNodeToIndexInfo, splits, nodeStack, timer, nodeIdCache)
        timer.stop("findBestSplits")
      }
  
      baggedInput.unpersist()
  
      timer.stop("total")
  
      logInfo("Internal timing for DecisionTree:")
      logInfo(s"$timer")
  
      // Delete any remaining checkpoints used for node Id cache.
      if (nodeIdCache.nonEmpty) {
        try {
          nodeIdCache.get.deleteAllCheckpoints()
        } catch {
          case e: IOException =>
            logWarning(s"delete all checkpoints failed. Error reason: ${e.getMessage}")
        }
      }
  
      val numFeatures = metadata.numFeatures
  
      parentUID match {
        case Some(uid) =>
          if (strategy.algo == OldAlgo.Classification) {
            topNodes.map { rootNode =>
              new DecisionTreeClassificationModel(uid, rootNode.toNode, numFeatures,
                strategy.getNumClasses)
            }
          } else {
            topNodes.map { rootNode =>
              new DecisionTreeRegressionModel(uid, rootNode.toNode, numFeatures)
            }
          }
        case None =>
          if (strategy.algo == OldAlgo.Classification) {
            topNodes.map { rootNode =>
              new DecisionTreeClassificationModel(rootNode.toNode, numFeatures,
                strategy.getNumClasses)
            }
          } else {
            topNodes.map(rootNode => new DecisionTreeRegressionModel(rootNode.toNode, numFeatures))
          }
      }
    }
  ```

- `findSplits`：针对每个特征，每个**切分点**构建`Split`对象，包括连续特征，有序类别特征，无序类别特征。注意，该方法使用样本数据集并非是**直接**对数据集进行切分，而是采样部分样本从而计算合理的切分方法保存起来。

  ```scala
  protected[tree] def findSplits(
      input: RDD[LabeledPoint],
      metadata: DecisionTreeMetadata,
      seed: Long): Array[Array[Split]] = {
  
      val numFeatures = metadata.numFeatures
  
      // Sample the input only if there are continuous features.
      val continuousFeatures = Range(0, numFeatures).filter(metadata.isContinuous)
      val sampledInput = if (continuousFeatures.nonEmpty) {
          // Calculate the number of samples for approximate quantile calculation.
          val requiredSamples = math.max(metadata.maxBins * metadata.maxBins, 10000)
          val fraction = if (requiredSamples < metadata.numExamples) {
              requiredSamples.toDouble / metadata.numExamples
          } else {
              1.0
          }
          input.sample(withReplacement = false, fraction, new XORShiftRandom(seed).nextInt())
      } else {
          input.sparkContext.emptyRDD[LabeledPoint]
      }
  
      findSplitsBySorting(sampledInput, metadata, continuousFeatures)
  }
  ```

  - 首先，如果有连续特征，则进行样本的降采样。降采样后的样本数为：$max(maxBins^{2},10000)$

  - 其次，对于降采样后的样本执行`findSplitsBySorting`，其中`continuousFeatures`作为参数传入

    ```scala
    private def findSplitsBySorting(
        input: RDD[LabeledPoint],
        metadata: DecisionTreeMetadata,
        continuousFeatures: IndexedSeq[Int]): Array[Array[Split]] = {
    
        val continuousSplits: scala.collection.Map[Int, Array[Split]] = {
            // reduce the parallelism for split computations when there are less
            // continuous features than input partitions. this prevents tasks from
            // being spun up that will definitely do no work.
            val numPartitions = math.min(continuousFeatures.length, input.partitions.length)
    
            input
            .flatMap(point => continuousFeatures.map(idx => (idx, point.features(idx))))
            .groupByKey(numPartitions)
            .map { case (idx, samples) =>
                val thresholds = findSplitsForContinuousFeature(samples, metadata, idx)
                val splits: Array[Split] = thresholds.map(thresh => new ContinuousSplit(idx, thresh))
                logDebug(s"featureIndex = $idx, numSplits = ${splits.length}")
                (idx, splits)
            }.collectAsMap()
        }
    
        val numFeatures = metadata.numFeatures
        val splits: Array[Array[Split]] = Array.tabulate(numFeatures) {
            // 针对每个特征idx numFeatures，作以下判断
            
            // 如果是连续特征，那么会使用上面计算的连续特征的Split
            case i if metadata.isContinuous(i) =>
            val split = continuousSplits(i)
            metadata.setNumSplits(i, split.length)
            split
    
            // 如果是无序的类别变量
            case i if metadata.isCategorical(i) && metadata.isUnordered(i) =>
            // Unordered features
            // 2^(maxFeatureValue - 1) - 1 combinations
            // 利用DecisionTreeMetaData类计算第i个类别idx的featureArity
            val featureArity = metadata.featureArity(i)
            Array.tabulate[Split](metadata.numSplits(i)) { splitIndex =>
                val categories = extractMultiClassCategories(splitIndex + 1, featureArity)
                new CategoricalSplit(i, categories.toArray, featureArity)
            }
    
            case i if metadata.isCategorical(i) =>
            // Ordered features
            //   Splits are constructed as needed during training.
            Array.empty[Split]
        }
        splits
    }
    ```

  - 这里针对`continuousFeature`，计算其Split。该变量的存储方式为`Map[featureId, Array[Split]]`，即某个连续特征id对应的Split对象

    - 将采样后的数据集RDD按照特征类型进行`groupBy`，得到的group为每个特征维度所有样本的集合。`groupBy`的时候限制`numPartitions`为$min(连续特征数量，输入RDD的partition数量)$

    - 对每个特征的所有样本，调用`findSplitsForContinuousFeature`函数获取连续特征的Split

      ```scala
      private[tree] def findSplitsForContinuousFeature(
          featureSamples: Iterable[Double],
          metadata: DecisionTreeMetadata,
          featureIndex: Int): Array[Double] = {
      
          val splits = if (featureSamples.isEmpty) {
              Array.empty[Double]
          } else {
              // 根据DecisionTreeMetaData对象计算featureIndex对应的特征需要划分多少个Splits
              val numSplits = metadata.numSplits(featureIndex)
      
              // get count for each distinct value
              // 统计连续变量不同值的数量，存为(val1 -> cnt1，val2 -> cnt2)的格式
              // 总共的样本数为numSamples
              val (valueCountMap, numSamples) = featureSamples.foldLeft((Map.empty[Double, Int], 0)) {
                  case ((m, cnt), x) =>
                  (m + ((x, m.getOrElse(x, 0) + 1)), cnt + 1)
              }
              // sort distinct values
              // 根据连续特征的值从小到大排序
              val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray
      
              // if possible splits is not enough or just enough, just return all possible splits
              val possibleSplits = valueCounts.length - 1
              if (possibleSplits <= numSplits) {
                  // 如果连续变量不同取值数量<=预计要切分的Split数量，那么就按照这些不同取值来Split了，init函数取slice(0,length-1)，即去除数组最后一个元素得到的数组
                  valueCounts.map(_._1).init
              } else {
                  // stride between splits
                  // 计算每个Split大致的样本数，根据这个Stride一步一步计算后面的Split
                  val stride: Double = numSamples.toDouble / (numSplits + 1)
                  logDebug("stride = " + stride)
      
                  // 后面的切割逻辑原理如下图所示
                  // iterate `valueCount` to find splits
                  val splitsBuilder = mutable.ArrayBuilder.make[Double]
                  var index = 1
                  // currentCount: sum of counts of values that have been visited
                  var currentCount = valueCounts(0)._2
                  // targetCount: target value for `currentCount`.
                  // If `currentCount` is closest value to `targetCount`,
                  // then current value is a split threshold.
                  // After finding a split threshold, `targetCount` is added by stride.
                  var targetCount = stride
                  while (index < valueCounts.length) {
                      val previousCount = currentCount
                      currentCount += valueCounts(index)._2
                      val previousGap = math.abs(previousCount - targetCount)
                      val currentGap = math.abs(currentCount - targetCount)
                      // If adding count of current value to currentCount
                      // makes the gap between currentCount and targetCount smaller,
                      // previous value is a split threshold.
                      if (previousGap < currentGap) {
                          splitsBuilder += valueCounts(index - 1)._1
                          targetCount += stride
                      }
                      index += 1
                  }
      
                  splitsBuilder.result()
              }
          }
          splits
      }
      ```

      ![](C:\Users\wangkai23\Desktop\作图\随机森林连续变量Split图解.PNG)

  - 针对无序的类别特征，如果有`numSplits`个切分，也会有这么多的`Split`。在该版本的实现中，作者对于不同Split包含的组合种类，用二进制的表示来获得。

    ![](C:\Users\WANGKA~1\AppData\Local\Temp\1537929286931.png)

    ```scala
    private[tree] def extractMultiClassCategories(
        input: Int,
        maxFeatureValue: Int): List[Double] = {
        var categories = List[Double]()
        var j = 0
        var bitShiftedInput = input
        while (j < maxFeatureValue) {
            if (bitShiftedInput % 2 != 0) {
                // updating the list of categories.
                categories = j.toDouble :: categories
            }
            // Right shift by one
            bitShiftedInput = bitShiftedInput >> 1
            j += 1
        }
        categories
    }
    ```

- `TreePoint.convertToTreeRDD`：核心作用是将一个`RDD of LabeledPoint`转化为`RDD of TreePoint`

  `TreePoint`数据结构如下：

  ```scala
  private[spark] class TreePoint(val label: Double, val binnedFeatures: Array[Int])
  ```

  label为样本标签，`binnedFeatures`的长度和特征的长度一样，其中每个元素为该特征值对应的Bin的索引

  转化的代码如下：

  ```scala
  def convertToTreeRDD(
      input: RDD[LabeledPoint],
      splits: Array[Array[Split]],
      metadata: DecisionTreeMetadata): RDD[TreePoint] = {
      // Construct arrays for featureArity for efficiency in the inner loop.
      // 该实现中规定：如果是连续变量，其arity为0
      val featureArity: Array[Int] = new Array[Int](metadata.numFeatures)
      var featureIndex = 0
      while (featureIndex < metadata.numFeatures) {
          featureArity(featureIndex) = metadata.featureArity.getOrElse(featureIndex, 0)
          featureIndex += 1
      }
      val thresholds: Array[Array[Double]] = featureArity.zipWithIndex.map { case (arity, idx) =>
          // 若arity为0，则为连续变量，取该连续特征对应的所有Split中的Threshold，
          // 该连续特征对应的Array of Threshold会被后面用于分bin得到索引
          if (arity == 0) {
              splits(idx).map(_.asInstanceOf[ContinuousSplit].threshold)
          } else {
              // 类别变量返回一个占位用的数组，因为类别变量会直接用类别取值作为bin的索引
              Array.empty[Double]
          }
      }
      input.map { x =>
          // 核心转化函数  LabeledPoint => TreePoint
          TreePoint.labeledPointToTreePoint(x, thresholds, featureArity)
      }
  }
  ```

  `LabeledPointToTreePoint`转化函数的逻辑如下：

  ```scala
  private def labeledPointToTreePoint(
      labeledPoint: LabeledPoint,
      thresholds: Array[Array[Double]],
      featureArity: Array[Int]): TreePoint = {
      val numFeatures = labeledPoint.features.size
      // 存储每个特征对应的bin索引
      val arr = new Array[Int](numFeatures)
      var featureIndex = 0
      // 对于每个特征进行遍历
      while (featureIndex < numFeatures) {
          // 利用findBin找到样本LabeledPoint在特征featureIndex上的bins索引
          arr(featureIndex) = findBin(featureIndex, labeledPoint, featureArity(featureIndex), thresholds(featureIndex))
          featureIndex += 1
      }
      // 构建一个TreePoint对象
      new TreePoint(labeledPoint.label, arr)
  }
  ```

  此处核心是`findBin`函数，逻辑如下：

  ```scala
  private def findBin(
      featureIndex: Int,
      labeledPoint: LabeledPoint,
      featureArity: Int,
      thresholds: Array[Double]): Int = {
      // 样本labeledPoint在featureIndex处对应的featureValue值
      val featureValue = labeledPoint.features(featureIndex)
  
      // featureArity为0，表示该特征为连续变量
      if (featureArity == 0) {
          // 根据该特征值，二分查找找到该数值所在的区间的index
          val idx = java.util.Arrays.binarySearch(thresholds, featureValue)
          if (idx >= 0) {
              idx
          } else {
              // 负值表示没有找到
              -idx - 1
          }
      } else {
  		// 离散变量的特征取值即为索引
          featureValue.toInt
      }
  }
  ```

- `BaggedPoint.convertToBaggedRDD`：

  `BaggedPoint`数据结构如下：

  ```scala
  private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])
  ```

  `datum`为一个属于`Datum`类型的变量，`Datum`是一个泛型的类型变量。`subsampleWeights`为一个Array数组，比如`[1.0, 2.0 ,0.0]`表示数据集被随机采样了3次`(numSubsamples)`，每一次的数据集采样`datum`被分别重复采到了`1,2,0`次。注意，如果是没有放回的采样`sample without replacement`，那么每个样本最多只能被采样一次，即数组元素值不会大于`1`。

  核心代码逻辑如下：

  ```scala
  def convertToBaggedRDD[Datum] (
      input: RDD[Datum],
      subsamplingRate: Double,
      numSubsamples: Int,
      withReplacement: Boolean,
      seed: Long = Utils.random.nextLong()): RDD[BaggedPoint[Datum]] = {
      if (withReplacement) {
          convertToBaggedRDDSamplingWithReplacement(input, subsamplingRate, numSubsamples, seed)
      } else {
          if (numSubsamples == 1 && subsamplingRate == 1.0) {
              convertToBaggedRDDWithoutSampling(input)
          } else {
              convertToBaggedRDDSamplingWithoutReplacement(input, subsamplingRate, numSubsamples, seed)
          }
      }
  }
  ```

  可以看到根据参数的不同，有三种采样方法，分别是：

  - `convertToBaggedRDDSamplingWithReplacement`：有放回采样。具体逻辑如下。

    ```scala
    private def convertToBaggedRDDSamplingWithReplacement[Datum] (
        input: RDD[Datum],
        subsample: Double,
        numSubsamples: Int,
        seed: Long): RDD[BaggedPoint[Datum]] = {
        input.mapPartitionsWithIndex { (partitionIndex, instances) =>
            // Use random seed = seed + partitionIndex + 1 to make generation reproducible.
            val poisson = new PoissonDistribution(subsample)
            poisson.reseedRandomGenerator(seed + partitionIndex + 1)
            instances.map { instance =>
                val subsampleWeights = new Array[Double](numSubsamples)
                var subsampleIndex = 0
                while (subsampleIndex < numSubsamples) {
                    subsampleWeights(subsampleIndex) = poisson.sample()
                    subsampleIndex += 1
                }
                new BaggedPoint(instance, subsampleWeights)
            }
        }
    }
    ```

    考虑到可复现的能力，设置随机种子。保证同一个partition的所有样本具有相同的随机种子。因为有放回抽样满足泊松分布，因此对于每个样本只需要从泊松分布中随机采样数值作为该样本可能出现的采样重复次数即可。采样`numSubsamples`次，最终一个样本对应一个`BaggedPoint`对象。`subsamplingRate`在泊松分布的参数中体现出来

  - `convertToBaggedRDDSamplingWithoutReplacement`：无放回采样。具体逻辑如下：

    ```scala
    private def convertToBaggedRDDSamplingWithoutReplacement[Datum] (
        input: RDD[Datum],
        subsamplingRate: Double,
        numSubsamples: Int,
        seed: Long): RDD[BaggedPoint[Datum]] = {
        input.mapPartitionsWithIndex { (partitionIndex, instances) =>
            // Use random seed = seed + partitionIndex + 1 to make generation reproducible.
            val rng = new XORShiftRandom
            rng.setSeed(seed + partitionIndex + 1)
            instances.map { instance =>
                val subsampleWeights = new Array[Double](numSubsamples)
                var subsampleIndex = 0
                while (subsampleIndex < numSubsamples) {
                    val x = rng.nextDouble()
                    subsampleWeights(subsampleIndex) = {
                        if (x < subsamplingRate) 1.0 else 0.0
                    }
                    subsampleIndex += 1
                }
                new BaggedPoint(instance, subsampleWeights)
            }
        }
    }
    ```

    `XORShiftRandom`是一个`uniform`分布随机生成器，其`nextDouble`方法均匀随机的生成一个`[0,1]`的数，因为是均匀分布，所以只需要和`subsamplingRate`作对比即可。

  - `convertToBaggedRDDWithoutSampling`：无需采样。

    ```scala
    private def convertToBaggedRDDWithoutSampling[Datum] (
        input: RDD[Datum]): RDD[BaggedPoint[Datum]] = {
        input.map(datum => new BaggedPoint(datum, Array(1.0)))
    }
    ```

    因为没有采用，所以可以认为采样一次，并且这次必中。

- 执行`BaggedPoint.convertToBaggedRDD`

  ```scala
  val baggedInput = BaggedPoint.convertToBaggedRDD(treeInput, strategy.subsamplingRate, 									numTrees, withReplacement, seed)
  				.persist(StorageLevel.MEMORY_AND_DISK)
  ```

  该步就是对数据集进行Bagging操作。输入为`RDD of TreePoint`，最终转化为`RDD of BaggedPoint`，其中`BaggedPoint`里的Datum为转化前的`TreePoint`样本。该树随机抽样了`numTrees`次，即每棵树对应一个随机抽样，每棵树在抽样时的降采样率为`subsamplingRate`，是否有放回也通过`withReplacement`进行设置。存储采样优先内存存储

- 目前支持的最大深度不能超过30，后面会解释为何最多只能支持30层深度

  ```scala
  require(maxDepth <= 30,
        s"DecisionTree currently only supports maxDepth <= 30, but was given maxDepth = $maxDepth.")
  ```

- 指定`maxMemoryUsage`。树在aggregate sufficient statistics的时候会针对一组节点进行收集，该参数用于精确指定分给该任务的内存数量。这里由`Mb`转为`byte`

  ```scala
  val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024L
  ```

- 训练树的核心存储变量`nodeStatck`。

  ```scala
  val nodeStack = new mutable.Stack[(Int, LearningNode)]
  ```

  该数据结构存储的数据为`treeIndex -> node`，即树的索引到当前需要训练的节点映射。如果当前取出的节点分裂，则会将分裂后形成的新节点加入该数据结构中。采用`Stack`保证父节点先于子节点执行，并且保证很多棵树可以同时进行训练。同时采用`Stack`的结构保证每轮迭代优先训练同一棵树，这样会相应的减少每轮跌倒由`driver`端发送到`worker`端的树的数量。

  整体训练思路为**尽量不要移动数据集，移动树与函数**，即每轮迭代将当前的树（其实是树当前的节点索引与特征信息）**广播**到各个`worker`中进行`aggregate`操作，收集用于后续树更新的`sufficient statistics`，最后将经过数据集更新后的一些计算数据`group`或者`collect`然后参数树的更新

- 所有树的根节点存储变量`topNodes`

  ```scala
  val topNodes = Array.fill[LearningNode](numTrees)(LearningNode.emptyNode(nodeIndex = 1))
  ```

  所有树的根节点初始化`nodeIndex`为1，训练中的节点类型为`LearningNode`，根节点初始化为空节点。该变量存储的所有根节点用来构建随机森林模型，后面的预测也是根据根节点递归的得到最后的预测值。

  ```scala
  Range(0, numTrees).foreach(treeIndex => nodeStack.push((treeIndex, topNodes(treeIndex))))
  ```

  初始化根节点后会将这些根节点放入到训练用的`nodeStack`中，作为训练的开始。

- 训练主循环逻辑

  ```scala
  while (nodeStack.nonEmpty) {
      // Collect some nodes to split, and choose features for each node (if subsampling).
      // Each group of nodes may come from one or multiple trees, and at multiple levels.
      val (nodesForGroup, treeToNodeToIndexInfo) =
      RandomForest.selectNodesToSplit(nodeStack, maxMemoryUsage, metadata, rng)
  
      // Only send trees to worker if they contain nodes being split this iteration.
      val topNodesForGroup: Map[Int, LearningNode] =
      nodesForGroup.keys.map(treeIdx => treeIdx -> topNodes(treeIdx)).toMap
  
      RandomForest.findBestSplits(baggedInput, metadata, topNodesForGroup, nodesForGroup,
                                  treeToNodeToIndexInfo, splits, nodeStack, timer, nodeIdCache)
  
  }
  ```

  - 终止条件：`nodeStack`已经没有需要分裂的节点

  - `RandomForest.selectNodesToSplit`：抽取一些`nodeStack`中尚有的可能需要分裂的节点组成一组，同时如果需要列采样还会随机采样一部分候选分裂特征。该函数获得的一些节点将会在后面被`broadcast`到`worker`端指导样本的`aggregate`操作。具体逻辑如下：

    ```scala
    private[tree] def selectNodesToSplit(
        nodeStack: mutable.Stack[(Int, LearningNode)],
        maxMemoryUsage: Long,
        metadata: DecisionTreeMetadata,
        rng: Random): (Map[Int, Array[LearningNode]], Map[Int, Map[Int, NodeIndexInfo]]) = {
        // Collect some nodes to split:
        //  nodesForGroup(treeIndex) = nodes to split
        val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[LearningNode]]()
        val mutableTreeToNodeToIndexInfo =
        new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]()
        var memUsage: Long = 0L
        var numNodesInGroup = 0
        // If maxMemoryInMB is set very small, we want to still try to split 1 node,
        // so we allow one iteration if memUsage == 0.
        // 从nodeStack中按照top to bottom获取节点，直到内存超过预设的最大内存使用或者取完为止
        // 当然如果第一个节点不管有没有超过最大内容使用都会取出来，保证至少有一个节点用于分裂
        while (nodeStack.nonEmpty && (memUsage < maxMemoryUsage || memUsage == 0)) {
            // 先peek，后续会判断若当前已使用内存量加上该节点的占用内存超过预设，则不取出来；否则添加进组中
            val (treeIndex, node) = nodeStack.top
            // Choose subset of features for node (if subsampling).
            // 精确的内存占用计算考虑到了列采样，列采样后会相应比例的减少内存占用，因此列采样在内存计算之前
            val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) {
    Some(SamplingUtils.reservoirSampleAndCount(Range(0,metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)
            } else {
                None
            }
            // Check if enough memory remains to add this node to the group.
            // 真正的内存占用计算函数
            val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L
            if (memUsage + nodeMemUsage <= maxMemoryUsage || memUsage == 0) {
                nodeStack.pop()
                mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=
                node
                mutableTreeToNodeToIndexInfo
                .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)
                = new NodeIndexInfo(numNodesInGroup, featureSubset)
            }
            numNodesInGroup += 1
            memUsage += nodeMemUsage
        }
        if (memUsage > maxMemoryUsage) {
            // If maxMemoryUsage is 0, we should still allow splitting 1 node.
            logWarning(s"Tree learning is using approximately $memUsage bytes per iteration, which" +
                       s" exceeds requested limit maxMemoryUsage=$maxMemoryUsage. This allows splitting" +
                       s" $numNodesInGroup nodes in this iteration.")
        }
        // Convert mutable maps to immutable ones.
        val nodesForGroup: Map[Int, Array[LearningNode]] =
        mutableNodesForGroup.mapValues(_.toArray).toMap
        val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap
        (nodesForGroup, treeToNodeToIndexInfo)
    }
    ```

    - 可以看到，该函数返回两个变量，分别为：

      ```scala
      // treeIndex -> nodes in tree
      // 该变量保存随机采样到的一些节点，并按照以树id为key存储成Map格式
      val nodesForGroup: Map[Int, Array[LearningNode]]
      // treeIndex -> [(global) node index -> nodeIndexInfo(group node index,feature indices)]
      val treeToNodeToIndexInfo:  Map[Int, Map[Int, NodeIndexInfo]])
      ```

      `NodeIndexInfo`类存以下两个变量：该节点在刚抽样到的一组中的index和列采样的特征索引集合

      ```scala
      private[tree] class NodeIndexInfo(
          val nodeIndexInGroup: Int,
          val featureSubset: Option[Array[Int]]) extends Serializable
      ```

    - 声明可变集合

      ```scala
          val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[LearningNode]]()
          val mutableTreeToNodeToIndexInfo =
            new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]()
      ```

      在本轮迭代中，需要不断根据内存的使用情况确定最终存储节点的多少，因此需要用可变集合。但是后续在训练时各个`worker`节点需要该集合，为了保证数据一致性会使用不可变集合。所以得到完整的存储节点集合后会有步转化的操作

    - 列采样采用的是蓄水池采样，该方法通常用于未知长度海量数据集采样，这里是对`[0，numFeatures)`的特征索引随机等概率不重复地采样`numFeaturesPerNode`个样本，得到采样后的特征索引`Array`。

      ```scala
            val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) {
              Some(SamplingUtils.reservoirSampleAndCount(Range(0,
                metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)
            } else {
              None
            }
      ```

      这里约定如果不需要特征列采样，则将`featureSubset`置为`None`。

      **注意**:每轮迭代，都会重新进行列采样！

    - `RandomForest.aggregateSizeForNode`：计算对样本进行`bin aggregate`操作所需要的内存占用。

      ```scala
        private def aggregateSizeForNode(
            metadata: DecisionTreeMetadata,
            featureSubset: Option[Array[Int]]): Long = {
          val totalBins = if (featureSubset.nonEmpty) {
            featureSubset.get.map(featureIndex => metadata.numBins(featureIndex).toLong).sum
          } else {
            metadata.numBins.map(_.toLong).sum
          }
          if (metadata.isClassification) {
            metadata.numClasses * totalBins
          } else {
            3 * totalBins
          }
        }
      ```

      `totalBins`为所有特征的分箱数之和。对于每个分箱，如果是分类任务，后续在`aggregate`操作时会有针对每个类别的统计计数操作，该`sufficient statitics`用于计算信息熵或者基尼系数；而对于回归任务，会存储计数，求和，平方和三个`sufficient statistics`，这三个参数可以用来计算平方误差，即`Variance`。

      因为每个`statistics`值都存为`Double`类型，因此最后计算结果还会$\times 8$

      ```scala
      val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L
      ```

    - 结果赋值

      ```scala
            if (memUsage + nodeMemUsage <= maxMemoryUsage || memUsage == 0) {
              nodeStack.pop()
              mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=
                node
              mutableTreeToNodeToIndexInfo
                .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)
                = new NodeIndexInfo(numNodesInGroup, featureSubset)
            }
      ```

      加上`nodeStack`的top元素如果内存使用并未超掉，则弹出该元素。将该节点存入`mutableNodesForGroup`中，若该变量还尚无treeIndex的树，则会创建该树，同时添加到`value`的`ArrayBuffer`中去；若有则直接取出并添加。同时填充`mutableTreeToNodeToIndexInfo`变量，这里的`numNodesInGroup`即在组中的index是根据添加的顺序临时确定的。每轮迭代都是临时确定的。

    - 可变到不可变集合的转化

      ```scala
          val nodesForGroup: Map[Int, Array[LearningNode]] =
            mutableNodesForGroup.mapValues(_.toArray).toMap
          val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap
          (nodesForGroup, treeToNodeToIndexInfo)
      ```

      利用`toMap`操作将可变Map转成不可变Map。注意，由于`mutableTreeToNodeToIndexInfo`嵌套了两层可变Map，因此需要执行两次`toMap`操作

  - 获取需要发送到`worker`的树

    ```scala
          val topNodesForGroup: Map[Int, LearningNode] =
            nodesForGroup.keys.map(treeIdx => treeIdx -> topNodes(treeIdx)).toMap
    ```

    遍历`nodesForGroup`，即需要处理的节点，然后将节点对应的树（树的根节点，两者在实现层面语义一致）找出来，后续会将该参数发送到`worker`节点方便`aggregate sufficient statistics`操作

  - `RandomForest.findBestSplits`：最核心的寻找最优切分函数

    ```scala
      private[tree] def findBestSplits(
          input: RDD[BaggedPoint[TreePoint]],
          metadata: DecisionTreeMetadata,
          topNodesForGroup: Map[Int, LearningNode],
          nodesForGroup: Map[Int, Array[LearningNode]],
          treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]],
          splits: Array[Array[Split]],
          nodeStack: mutable.Stack[(Int, LearningNode)],
          timer: TimeTracker = new TimeTracker,
          nodeIdCache: Option[NodeIdCache] = None): Unit = {
    
        /*
         * The high-level descriptions of the best split optimizations are noted here.
         *
         * *Group-wise training*
         * We perform bin calculations for groups of nodes to reduce the number of
         * passes over the data.  Each iteration requires more computation and storage,
         * but saves several iterations over the data.
         *
         * *Bin-wise computation*
         * We use a bin-wise best split computation strategy instead of a straightforward best split
         * computation strategy. Instead of analyzing each sample for contribution to the left/right
         * child node impurity of every split, we first categorize each feature of a sample into a
         * bin. We exploit this structure to calculate aggregates for bins and then use these aggregates
         * to calculate information gain for each split.
         *
         * *Aggregation over partitions*
         * Instead of performing a flatMap/reduceByKey operation, we exploit the fact that we know
         * the number of splits in advance. Thus, we store the aggregates (at the appropriate
         * indices) in a single array for all bins and rely upon the RDD aggregate method to
         * drastically reduce the communication overhead.
         */
    
        // numNodes:  Number of nodes in this group
        val numNodes = nodesForGroup.values.map(_.length).sum
    
        /**
         * Performs a sequential aggregation over a partition for a particular tree and node.
         *
         * For each feature, the aggregate sufficient statistics are updated for the relevant
         * bins.
         *
         * @param treeIndex Index of the tree that we want to perform aggregation for.
         * @param nodeInfo The node info for the tree node.
         * @param agg Array storing aggregate calculation, with a set of sufficient statistics
         *            for each (node, feature, bin).
         * @param baggedPoint Data point being aggregated.
         */
        def nodeBinSeqOp(
            treeIndex: Int,
            nodeInfo: NodeIndexInfo,
            agg: Array[DTStatsAggregator],
            baggedPoint: BaggedPoint[TreePoint]): Unit = {
          if (nodeInfo != null) {
            val aggNodeIndex = nodeInfo.nodeIndexInGroup
            val featuresForNode = nodeInfo.featureSubset
            val instanceWeight = baggedPoint.subsampleWeights(treeIndex)
            if (metadata.unorderedFeatures.isEmpty) {
              orderedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, instanceWeight, featuresForNode)
            } else {
              mixedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, splits,
                metadata.unorderedFeatures, instanceWeight, featuresForNode)
            }
            agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)
          }
        }
    
        /**
         * Performs a sequential aggregation over a partition.
         *
         * Each data point contributes to one node. For each feature,
         * the aggregate sufficient statistics are updated for the relevant bins.
         *
         * @param agg  Array storing aggregate calculation, with a set of sufficient statistics for
         *             each (node, feature, bin).
         * @param baggedPoint   Data point being aggregated.
         * @return  agg
         */
        def binSeqOp(
            agg: Array[DTStatsAggregator],
            baggedPoint: BaggedPoint[TreePoint]): Array[DTStatsAggregator] = {
          treeToNodeToIndexInfo.foreach { case (treeIndex, nodeIndexToInfo) =>
            val nodeIndex =
              topNodesForGroup(treeIndex).predictImpl(baggedPoint.datum.binnedFeatures, splits)
            nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)
          }
          agg
        }
    
        /**
         * Do the same thing as binSeqOp, but with nodeIdCache.
         */
        def binSeqOpWithNodeIdCache(
            agg: Array[DTStatsAggregator],
            dataPoint: (BaggedPoint[TreePoint], Array[Int])): Array[DTStatsAggregator] = {
          treeToNodeToIndexInfo.foreach { case (treeIndex, nodeIndexToInfo) =>
            val baggedPoint = dataPoint._1
            val nodeIdCache = dataPoint._2
            val nodeIndex = nodeIdCache(treeIndex)
            nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)
          }
    
          agg
        }
    
        /**
         * Get node index in group --> features indices map,
         * which is a short cut to find feature indices for a node given node index in group.
         */
        def getNodeToFeatures(
            treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]]): Option[Map[Int, Array[Int]]] = {
          if (!metadata.subsamplingFeatures) {
            None
          } else {
            val mutableNodeToFeatures = new mutable.HashMap[Int, Array[Int]]()
            treeToNodeToIndexInfo.values.foreach { nodeIdToNodeInfo =>
              nodeIdToNodeInfo.values.foreach { nodeIndexInfo =>
                assert(nodeIndexInfo.featureSubset.isDefined)
                mutableNodeToFeatures(nodeIndexInfo.nodeIndexInGroup) = nodeIndexInfo.featureSubset.get
              }
            }
            Some(mutableNodeToFeatures.toMap)
          }
        }
    
        // array of nodes to train indexed by node index in group
        val nodes = new Array[LearningNode](numNodes)
        nodesForGroup.foreach { case (treeIndex, nodesForTree) =>
          nodesForTree.foreach { node =>
            nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node
          }
        }
    
    
        // In each partition, iterate all instances and compute aggregate stats for each node,
        // yield a (nodeIndex, nodeAggregateStats) pair for each node.
        // After a `reduceByKey` operation,
        // stats of a node will be shuffled to a particular partition and be combined together,
        // then best splits for nodes are found there.
        // Finally, only best Splits for nodes are collected to driver to construct decision tree.
        val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)
        val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)
    
        val partitionAggregates: RDD[(Int, DTStatsAggregator)] = if (nodeIdCache.nonEmpty) {
          input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions { points =>
            // Construct a nodeStatsAggregators array to hold node aggregate stats,
            // each node will have a nodeStatsAggregator
            val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =>
              val featuresForNode = nodeToFeaturesBc.value.map { nodeToFeatures =>
                nodeToFeatures(nodeIndex)
              }
              new DTStatsAggregator(metadata, featuresForNode)
            }
    
            // iterator all instances in current partition and update aggregate stats
            points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _))
    
            // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,
            // which can be combined with other partition using `reduceByKey`
            nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator
          }
        } else {
          input.mapPartitions { points =>
            // Construct a nodeStatsAggregators array to hold node aggregate stats,
            // each node will have a nodeStatsAggregator
            val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =>
              val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>
                Some(nodeToFeatures(nodeIndex))
              }
              new DTStatsAggregator(metadata, featuresForNode)
            }
    
            // iterator all instances in current partition and update aggregate stats
            points.foreach(binSeqOp(nodeStatsAggregators, _))
    
            // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,
            // which can be combined with other partition using `reduceByKey`
            nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator
          }
        }
    
        val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) => a.merge(b)).map {
          case (nodeIndex, aggStats) =>
            val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>
              Some(nodeToFeatures(nodeIndex))
            }
    
            // find best split for each node
            val (split: Split, stats: ImpurityStats) =
              binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))
            (nodeIndex, (split, stats))
        }.collectAsMap()
    
    
        val nodeIdUpdaters = if (nodeIdCache.nonEmpty) {
          Array.fill[mutable.Map[Int, NodeIndexUpdater]](
            metadata.numTrees)(mutable.Map[Int, NodeIndexUpdater]())
        } else {
          null
        }
        // Iterate over all nodes in this group.
        nodesForGroup.foreach { case (treeIndex, nodesForTree) =>
          nodesForTree.foreach { node =>
            val nodeIndex = node.id
            val nodeInfo = treeToNodeToIndexInfo(treeIndex)(nodeIndex)
            val aggNodeIndex = nodeInfo.nodeIndexInGroup
            val (split: Split, stats: ImpurityStats) =
              nodeToBestSplits(aggNodeIndex)
            logDebug("best split = " + split)
    
            // Extract info for this node.  Create children if not leaf.
            val isLeaf =
              (stats.gain <= 0) || (LearningNode.indexToLevel(nodeIndex) == metadata.maxDepth)
            node.isLeaf = isLeaf
            node.stats = stats
            logDebug("Node = " + node)
    
            if (!isLeaf) {
              node.split = Some(split)
              val childIsLeaf = (LearningNode.indexToLevel(nodeIndex) + 1) == metadata.maxDepth
              val leftChildIsLeaf = childIsLeaf || (stats.leftImpurity == 0.0)
              val rightChildIsLeaf = childIsLeaf || (stats.rightImpurity == 0.0)
              node.leftChild = Some(LearningNode(LearningNode.leftChildIndex(nodeIndex),
                leftChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.leftImpurityCalculator)))
              node.rightChild = Some(LearningNode(LearningNode.rightChildIndex(nodeIndex),
                rightChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.rightImpurityCalculator)))
    
              if (nodeIdCache.nonEmpty) {
                val nodeIndexUpdater = NodeIndexUpdater(
                  split = split,
                  nodeIndex = nodeIndex)
                nodeIdUpdaters(treeIndex).put(nodeIndex, nodeIndexUpdater)
              }
    
              // enqueue left child and right child if they are not leaves
              if (!leftChildIsLeaf) {
                nodeStack.push((treeIndex, node.leftChild.get))
              }
              if (!rightChildIsLeaf) {
                nodeStack.push((treeIndex, node.rightChild.get))
              }
    
              logDebug("leftChildIndex = " + node.leftChild.get.id +
                ", impurity = " + stats.leftImpurity)
              logDebug("rightChildIndex = " + node.rightChild.get.id +
                ", impurity = " + stats.rightImpurity)
            }
          }
        }
    
        if (nodeIdCache.nonEmpty) {
          // Update the cache if needed.
          nodeIdCache.get.updateNodeIndices(input, nodeIdUpdaters, splits)
        }
      }
    ```

    由于分析`nodeIdCache`会增加复杂度，所以本次分析忽略该参数

    - 主要入参分析

      ```scala
        private[tree] def findBestSplits(
            input: RDD[BaggedPoint[TreePoint]],
            metadata: DecisionTreeMetadata,
            topNodesForGroup: Map[Int, LearningNode],
            nodesForGroup: Map[Int, Array[LearningNode]],
            treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]],
            splits: Array[Array[Split]],
            nodeStack: mutable.Stack[(Int, LearningNode)],
            timer: TimeTracker = new TimeTracker,
            nodeIdCache: Option[NodeIdCache] = None)
      ```

      - `input`:存在`worker`端的`RDD of BaggedPoint of Treepoint`，每个sample已经包含了`numTress`次的样本采样（采样包含还是不包含该样本），sample的每列特征值为该列特征bin化后的`bin index`
      - `metadata`：各种模型训练参数
      - `topNodesForGroup`：本轮迭代采样的这一组节点对应的所有的树（或根节点）
      - `nodesForGroup`：本轮要参与迭代的一组节点
      - `treeToNodeToIndexInfo`：本轮要参与迭代的一组节点的其它数据，组的临时index和列采样特征子集
      - `split`：所有特征，所有分割对应的`Split`对象
      - `nodeStack`：全局的可能需要分裂的节点集合，在该函数中如果计算到该节点还需要分裂，那么会添加到该变量中

    - 获取当前该组中所有的节点，保存为`nodes`变量

      ```scala
          // numNodes:  Number of nodes in this group
          val numNodes = nodesForGroup.values.map(_.length).sum    
      	// array of nodes to train indexed by node index in group
          val nodes = new Array[LearningNode](numNodes)
          nodesForGroup.foreach { case (treeIndex, nodesForTree) =>
            nodesForTree.foreach { node =>
              nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node
            }
          }
      ```

      首先统计所有`nodesForGroup`中节点数量保存为`numNodes`，初始化一个和该组样本量一致的用于保存该组节点的集合对象`nodes`，并填充值。nodes中索引节点的index和`treeToNodeToIndexInfo`中保存的`group index`一致

    - 获取所有节点特征

      ```scala
      val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)
      ```

      该函数具体实现如下：

      ```scala
          def getNodeToFeatures(
              treeToNodeToIndexInfo: Map[Int, Map[Int, NodeIndexInfo]]): Option[Map[Int, Array[Int]]] = {
            if (!metadata.subsamplingFeatures) {
                // 如果不降采样特征，那么返回的特征为None，这是之前约定的
              None
            } else {
              val mutableNodeToFeatures = new mutable.HashMap[Int, Array[Int]]()
              treeToNodeToIndexInfo.values.foreach { nodeIdToNodeInfo =>
                  // 每棵树
                nodeIdToNodeInfo.values.foreach { nodeIndexInfo =>
                    // 当前树中的每个节点
                  assert(nodeIndexInfo.featureSubset.isDefined)
                    // 存为：该节点对应的组index -> 该节点的特征子集
                  mutableNodeToFeatures(nodeIndexInfo.nodeIndexInGroup) = nodeIndexInfo.featureSubset.get
                }
              }
                // 转为不可变，并存为Option
              Some(mutableNodeToFeatures.toMap)
            }
          }
      ```

      该函数主要用于找到所有节点及其列采样特征子集。这里的节点index键值是组index，和上面的`nodes`的索引一致

    - 广播组节点特征子集，每个`worker`节点保存一份副本，而不是每个`task`，提高性能

      ```scala
      val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)
      ```

      每个`worker`会得到该特征子集，这是每轮迭代都会执行的操作，即每轮迭代`broadcast`一次，因为每轮迭代所有的这些组节点值，列采样值等都不一样

    - 对每个`partition`，计算其对应的`sufficient statistics`。为方便起见，本次分析全部忽略`nodeIdCache`版本

      ```scala
      val partitionAggregates: RDD[(Int, DTStatsAggregator)] =      
      		input.mapPartitions { points =>
              // Construct a nodeStatsAggregators array to hold node aggregate stats,
              // each node will have a nodeStatsAggregator
              val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =>
                val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>
                  Some(nodeToFeatures(nodeIndex))
                }
                new DTStatsAggregator(metadata, featuresForNode)
              }
      
              // iterator all instances in current partition and update aggregate stats
              points.foreach(binSeqOp(nodeStatsAggregators, _))
      
              // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs,
              // which can be combined with other partition using `reduceByKey`
              nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator
            }
      ```

      首先，`input.mapPartitions { points => }`表明`points`的操作发生在`worker`端，并且是同一个`partition`内。

      `Array.tabulate(numNodes)`定义了`numNodes`个`DTStatsAggregator`，每个元素的初始化由`Array`的`index`决定。对于`Array`中`nodeIndex`索引位置，找到`nodeToFeatures`在该位置的元素即为该节点id对应的特征子集，并用该特征子集和树模型训练元数据初始化`DTStatsAggregator`。可以看到在`worker`端初始化`DTStatsAggregator`的时候，用到了从`driver`端`broadcast`过来的节点特征变量`nodeToFeaturesBc`

      ```scala
      private[spark] class DTStatsAggregator(
          val metadata: DecisionTreeMetadata,
          featureSubset: Option[Array[Int]]) extends Serializable {
       
        val impurityAggregator: ImpurityAggregator = metadata.impurity match {
          case Gini => new GiniAggregator(metadata.numClasses)
          case Entropy => new EntropyAggregator(metadata.numClasses)
          case Variance => new VarianceAggregator()
          case _ => throw new IllegalArgumentException(s"Bad impurity parameter: ${metadata.impurity}")
        }
      }
      ```

      这是决策树的节点`sufficient statistics aggregator`，该对象会保存`driver`端传来的`featureSubset`。该对象里面保存一个真正干活的`ImpurityAggregator`对象，分别用来收集和分类，回归等损失函数相关的`sufficient statistics aggregator`

      **注意**：`DTStatsAggregator`是每轮迭代，每个数据集partition里，每个参与本轮更新的组中树节点有一个。

      对于该数据集`partition`里的每个点`points.foreach()`，执行`binSeqOp()`操作，这里是一份数据样本，组节点数个`aggregator`

      ```scala
          def binSeqOp(
              agg: Array[DTStatsAggregator],
              baggedPoint: BaggedPoint[TreePoint]): Array[DTStatsAggregator] = {
            treeToNodeToIndexInfo.foreach { case (treeIndex, nodeIndexToInfo) =>
              val nodeIndex =
                topNodesForGroup(treeIndex).predictImpl(baggedPoint.datum.binnedFeatures, splits)
                // 该样本最终落入的节点也是有可能不在本轮更新的nodeIndexToInfo若干节点中，如果不在其中，则置传入null
                // 该函数对于每棵树，每个节点，每个样本点进行操作
              nodeBinSeqOp(treeIndex, nodeIndexToInfo.getOrElse(nodeIndex, null), agg, baggedPoint)
            }
            agg
          }
      ```

      - 对于**每棵**树，预测当前样本点`baggedPoint`在树中位置，该位置用全局的`nodeIndex`表示。预测时函数接受`binnedFeature`，`predictImpl`是`LearningNode`的方法

        ```scala
          def predictImpl(binnedFeatures: Array[Int], splits: Array[Array[Split]]): Int = {
            if (this.isLeaf || this.split.isEmpty) {
                // 叶子节点或者当前节点尚未有可切分点，则样本点分到该节点处
                // 尚未有可切分点的情况比如一开始的根节点，第一次进入这里是没有可切分点的，则样本点会停留这里
              this.id
            } else {
                // 如果是可分割点，则找到之前运算保存的LearningNode中的split，该split可以决定之前确定的在当前节点处应该根据哪个特征，并且如何进入下一层节点
              val split = this.split.get
              val featureIndex = split.featureIndex
                // 节点在该特征上的值是否能决定往左分裂，根据该样本点在该特征上的值和总的所有分裂值情况进行比较得到
                // 该函数对于不同的特征类型，实现不一样
              val splitLeft = split.shouldGoLeft(binnedFeatures(featureIndex), splits(featureIndex))
              if (this.leftChild.isEmpty) {
                  // 如果当前节点已经确定好切分点且不为叶子节点（从上面知道），则获取当前叶子节点的左子节点索引返回
                // Not yet split. Return next layer of nodes to train
                if (splitLeft) {
                    // 分裂后左子节点，节点索引为this.id*2
                  LearningNode.leftChildIndex(this.id)
                } else {
                    // 分裂后右子节点，节点索引为this.id*2 + 1
                  LearningNode.rightChildIndex(this.id)
                }
              } else {
                  // 如果当前节点是内部节点且可分，并且已经分好，则进入对应的子节点继续寻找该样本应该所在的节点索引位置
                if (splitLeft) {
                  this.leftChild.get.predictImpl(binnedFeatures, splits)
                } else {
                  this.rightChild.get.predictImpl(binnedFeatures, splits)
                }
              }
            }
          }
        ```

        最终通过`predictImpl`函数得到样本点`baggedPoint`在当前已经部分构建好的树的节点索引位置，**注意**，该位置只得到其索引，`LearningNode`中的其它分裂参数是没有的，因为该索引位置的节点还尚未分裂，同时也不能确定是否能分裂，后面计算信息增益会决定能够分裂（比如根节点就是一个例子，一开始有这样的索引，但是里面的分裂相关参数都还未确定）

      - 对于**每棵树`treeIndex`**，**每个节点`nodeInfo`**，**每个样本点`baggedPoint`**执行`nodeBinSeqOp`，

        ```scala
            def nodeBinSeqOp(
                treeIndex: Int,
                nodeInfo: NodeIndexInfo,
                agg: Array[DTStatsAggregator],
                baggedPoint: BaggedPoint[TreePoint]): Unit = {
                // 如果treeIndex索引的树没有对应的NodeIndexInfo（即属于该树的节点），那么对于这样的树就无需任何处理
              if (nodeInfo != null) {
                  // 该节点对应的节点组index
                val aggNodeIndex = nodeInfo.nodeIndexInGroup
                  // 该节点对应的节点特征子集
                val featuresForNode = nodeInfo.featureSubset
                  // 该棵树在treeIndex次样本的降采样的情况，如果是无放回采样，那么该样本在该棵树上只有可能是0或者1，采到或者没采到
                val instanceWeight = baggedPoint.subsampleWeights(treeIndex)
                if (metadata.unorderedFeatures.isEmpty) {
                    // 特征全为有序特征，入参为：该节点对应的aggregator（之前的计算，一个节点对应一个aggregator），该样本点，该棵树的采样情况，以及列采样特征子集
                  orderedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, instanceWeight, featuresForNode)
                } else {
                    // 既有有序，也有无需特征
                  mixedBinSeqOp(agg(aggNodeIndex), baggedPoint.datum, splits,
                    metadata.unorderedFeatures, instanceWeight, featuresForNode)
                }
                agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)
              }
            }
        ```

        - `orderedBinSeqOp`：只有有序特征的`aggregate`方式。对某树某个节点所有特征，收集该样本在这些特征上所在bin的统计量

          ```scala
            private def orderedBinSeqOp(
                agg: DTStatsAggregator,
                treePoint: TreePoint,
                instanceWeight: Double,
                featuresForNode: Option[Array[Int]]): Unit = {
                // 计算aggregator信息和label有关
              val label = treePoint.label
          
              // Iterate over features.
              if (featuresForNode.nonEmpty) {
                  // nonEmpty则进行了列采样
                // Use subsampled features
                var featureIndexIdx = 0
                  // 遍历所有特征
                while (featureIndexIdx < featuresForNode.get.length) {
                    // 该样本点在该特征上binned后的取值，分箱值
                  val binIndex = treePoint.binnedFeatures(featuresForNode.get.apply(featureIndexIdx))
                    // 一个partition，一个node，一个agg，因此对于新样本的到来，agg调用update函数更新agg信息。最终agg会得到所有该partition、该node所有样本的agg汇总值
                  agg.update(featureIndexIdx, binIndex, label, instanceWeight)
                  featureIndexIdx += 1
                }
              } else {
                  // Empty表示未进行降采样
                // Use all features
                  // 未进行列采样则会使用metadata的总特征数进行遍历
                val numFeatures = agg.metadata.numFeatures
                var featureIndex = 0
                while (featureIndex < numFeatures) {
                  val binIndex = treePoint.binnedFeatures(featureIndex)
                  agg.update(featureIndex, binIndex, label, instanceWeight)
                  featureIndex += 1
                }
              }
            }
          ```

          **注意**：`featureIndexIdx`表示本轮迭代经过列采样后的特征索引；`featureIndex`为全局的特征索引。本轮迭代的`DTStatsAggregator`里的`allStats`里的特征数就是本次列采样后的特征数，并非全局的特征数

          这里`agg.update`会委派给`ImpurityAggregator`完成

          ```scala
            def update(featureIndex: Int, binIndex: Int, label: Double, instanceWeight: Double): Unit = {
                // 计算特征featureIndex，分箱值binIndex对应的allStats数组偏置
              val i = featureOffsets(featureIndex) + binIndex * statsSize
              impurityAggregator.update(allStats, i, label, instanceWeight)
            }
          ```

          `DTStatsAggregator`类中保存两个非常重要的变量`allStats`和`parentStats`，如下：

          ```scala
            private val allStats: Array[Double] = new Array[Double](allStatsSize)
            private val parentStats: Array[Double] = new Array[Double](statsSize)
          
          	// statsSize表示ImpurityAggregator存储sufficient statistics所需要的向量长度
          	// featureOffSets数组示意图如下图
            private val featureOffsets: Array[Int] = {
              numBins.scanLeft(0)((total, nBins) => total + statsSize * nBins)
            }
            private val allStatsSize: Int = featureOffsets.last
          ```

          `allStats`的存储示意图如下：

          ![](C:\Users\wangkai23\Desktop\作图\feature-bin-statsSize摊平示意.PNG)

          `impurityAggregator.update(allStats, i, label, instanceWeight)`就是`DTStatsAggregator`委派给`ImpurityAggregator`更新自己存储的`allStats`变量

          `impurityAggregator.update`是抽象类`ImpurityAggregator`的方法，对应三个信息增益的Impurity衡量指标，其`aggregator`也有相应的三种实现类：

          ```scala
          // 以下两个用于分类
          // EntropyAggregator和GiniAggregator的sufficient statistics需要numClasses个变量，存储该partition所有样本，当前树，其特定节点每个label的样本总数，如果是有放回采样，有些样本可能被采样多次。
          // 计算基尼系数和熵这些参数就够了
          private[spark] class EntropyAggregator(numClasses: Int)
            extends ImpurityAggregator(numClasses) with Serializable
          
          private[spark] class GiniAggregator(numClasses: Int)
            extends ImpurityAggregator(numClasses)
          
          // 用于回归
          // VarianceAggregator的sufficient statistics需要3个变量，分别存储总样本数量、样本值求和、样本值平方和。统计所有样本这三个统计量就可以计算最后的variance了。后面具体计算会推导
          private[spark] class VarianceAggregator()
            extends ImpurityAggregator(statsSize = 3)
          ```

          **注意**：子类调用了父类的构造器初始化其`statsSize`参数，即`sufficient statistics`所需要的参数个数。三个子类的不同点主要在于其`update`方法和`getCalculator`方法。对应三个信息增益的衡量指标，同样的还存在一个`ImpurityCalculator`父类和三个实现类：

          ```scala
          // 父类
          private[spark] abstract class ImpurityCalculator(val stats: Array[Double])
          
          // 子类
          private[spark] class EntropyCalculator(stats: Array[Double]) extends ImpurityCalculator(stats)
          
          private[spark] class GiniCalculator(stats: Array[Double]) extends ImpurityCalculator(stats)
          
          private[spark] class VarianceCalculator(stats: Array[Double]) extends ImpurityCalculator(stats)
          ```

          `Calculator`和`Aggregator`的区别在于：`Aggregator`主要用于统计`sufficient statistics`（其中，不同的信息增益衡量方法所需要的收集方法不同，才会出现类的分化，收集的维度是样本，具体实现中是一个partition里的样本），方便后面信息增益的计算；而`Calculator`则主要用于将通过`Aggregator`收集好的`sufficient statistics`（保存在`DTStatsAggregator`中的`allStats`）计算信息增益（同理，不同的信息增益衡量方法所需要的收集方法不同）

          三个`Aggregator`的`update`方法执行分别如下：

          ```scala
          // EntropyAggregator  
          	def update(allStats: Array[Double], offset: Int, label: Double, instanceWeight: Double): Unit = {
          	  // 将DTStatsAggregator中的allStats对应位置添加该样本的采样权重
                // offset前面已经计算，包含了featureIndex和bin两个坐标
                // 该函数是通过副作用起作用的，修改传入的allStates值
              allStats(offset + label.toInt) += instanceWeight
            }
          
          // GiniAggregator
            def update(allStats: Array[Double], offset: Int, label: Double, instanceWeight: Double): Unit = {
                // Gini和Entropy的所需sufficient statistics一致，只是计算方式不一样
              allStats(offset + label.toInt) += instanceWeight
            }
          
          // VarianceAggregator
            def update(allStats: Array[Double], offset: Int, label: Double, instanceWeight: Double): Unit = {
                // 可以看到，以下三个公式分别是求总数、总和、总平方和的增量公式
                // 在大数据处理中通常会采用增量计算方式，因为遍历一次耗时较多
              allStats(offset) += instanceWeight
              allStats(offset + 1) += instanceWeight * label
              allStats(offset + 2) += instanceWeight * label * label
            }
          ```

        - `mixedBinSeqOp`：有序和无序特征的`aggregate`，对某树某个节点所有特征，收集该样本在这些特征上所在bin的统计量

          ```scala
            private def mixedBinSeqOp(
                agg: DTStatsAggregator,
                treePoint: TreePoint,
                splits: Array[Array[Split]],
                unorderedFeatures: Set[Int],
                instanceWeight: Double,
                featuresForNode: Option[Array[Int]]): Unit = {
                // 统计特征量，是否进行列采样
              val numFeaturesPerNode = if (featuresForNode.nonEmpty) {
                // Use subsampled features
                featuresForNode.get.length
              } else {
                // Use all features
                agg.metadata.numFeatures
              }
              // Iterate over features.
              var featureIndexIdx = 0
              while (featureIndexIdx < numFeaturesPerNode) {
                  // 获取当前特征index的特征值，是否进行列采样
                val featureIndex = if (featuresForNode.nonEmpty) {
                  featuresForNode.get.apply(featureIndexIdx)
                } else {
                  featureIndexIdx
                }
                  // 当前特征是否是无序特征
                if (unorderedFeatures.contains(featureIndex)) {
                    // 如果为无序特征
                    
                  // Unordered feature
                    // 获取当前样本在当前特征上的bin特征
                  val featureValue = treePoint.binnedFeatures(featureIndex)
                    // 获取当前样本在当前特征上的allStats变量的offset
                  val leftNodeFeatureOffset = agg.getFeatureOffset(featureIndexIdx)
                  // Update the left or right bin for each split.
                    // 当前特征的分裂点总数以及分裂点Split
                  val numSplits = agg.metadata.numSplits(featureIndex)
                  val featureSplits = splits(featureIndex)
                  var splitIndex = 0
                    // 遍历所有分裂点，无序特征bin的个数就是分裂点个数，每个bin表示无序特征分裂后保存在左边节点的数据统计
                  while (splitIndex < numSplits) {
                      // 当前样本的当前特征在当前特征的Split上第splitIndex个分裂法是否分到左子树
                    if (featureSplits(splitIndex).shouldGoLeft(featureValue, featureSplits)) {
                        // 如果在当前切分法分到左子树，那么左子树的统计值会增加该样本的统计
                      agg.featureUpdate(leftNodeFeatureOffset, splitIndex, treePoint.label, instanceWeight)
                    }
                    splitIndex += 1
                  }
                } else {
                  // Ordered feature
                    // 有序特征和上述有序特征的收集方法一致
                  val binIndex = treePoint.binnedFeatures(featureIndex)
                  agg.update(featureIndexIdx, binIndex, treePoint.label, instanceWeight)
                }
                featureIndexIdx += 1
              }
            }
          ```

          其中，`Split`对象可以用来判断是否应该分到左边`shouldGoLeft`。针对类别特征和连续特征，其有两个实现方法:

          ```scala
          // 类别特征CategoricalSplit  
            override private[tree] def shouldGoLeft(binnedFeature: Int, splits: Array[Split]): Boolean = {
              if (isLeft) {
                  // categories是一个左集合，即落入该集合的样本如果要分裂都会往左走
                  // isLeft判断categories是否
                categories.contains(binnedFeature.toDouble)
              } else {
                !categories.contains(binnedFeature.toDouble)
              }
            }
          
          // 连续特征ContinuousSplit
            override private[tree] def shouldGoLeft(binnedFeature: Int, splits: Array[Split]): Boolean = {
              if (binnedFeature == splits.length) {
                // > last split, so split right
                  // 特征在最右边的bin，那么无论用哪种分割方式也会分到右子树
                false
              } else {
                val featureValueUpperBound = splits(binnedFeature).asInstanceOf[ContinuousSplit].threshold
                  // 当前样本的特征值binnedFeature所在的Split处的threshold值和假如要进行当前分割所得到的threshold作对比，得到如果采用当前分割那么会进入左集合还是右集合
                featureValueUpperBound <= threshold
              }
            }
          ```

          最终进行`agg.featureUpdate`，如下：

          ```scala
            def featureUpdate(
                featureOffset: Int,
                binIndex: Int,
                label: Double,
                instanceWeight: Double): Unit = {
              impurityAggregator.update(allStats, featureOffset + binIndex * statsSize,
                label, instanceWeight)
            }
          ```

          可以看到，针对每个`split`（无序特征，`split`也是`bin`），都会统计该`split`所对应的`sufficient statistics`

        - 更新该节点整体（未分裂）的`sufficient statistics`，后面用于计算分裂前的基尼系数

          ```scala
                  agg(aggNodeIndex).updateParent(baggedPoint.datum.label, instanceWeight)
          
            def updateParent(label: Double, instanceWeight: Double): Unit = {
              impurityAggregator.update(parentStats, 0, label, instanceWeight)
            }
          ```

          更新的是`DTStatsAggregator`中的`parentStats`变量

      至此，完成更新**当前partition**的数据集，所有**节点**所有**特征**所有**分裂**方式的`aggregate`操作。通过下面的`foreach`副作用更新了`nodeStatsAggregators`变量

      ```scala
              // iterator all instances in current partition and update aggregate stats
              points.foreach(binSeqOp(nodeStatsAggregators, _))
      ```

      下面将每个partition里的所有`aggregator`按照`nodeIndex`进行编号，存为`(nodeIndex, nodeAggregator)`的pair，代码如下：

      ```scala
              nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator
      ```

      该步方便了后续将所有partition相同的节点`aggregator`汇总

    - 所有partition的`sufficient statistics`汇总

      ```scala
          val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) => a.merge(b)).map {
            case (nodeIndex, aggStats) =>
              val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>
                Some(nodeToFeatures(nodeIndex))
              }
      
              // find best split for each node
              val (split: Split, stats: ImpurityStats) =
                binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))
              (nodeIndex, (split, stats))
          }.collectAsMap()
      ```

      `reduceByKey`按照节点id沿着所有partition进行汇总，汇总调用了`merge`方法，该方法在`DSTatsAggregator`中有定义，如下：

      ```scala
        def merge(other: DTStatsAggregator): DTStatsAggregator = {
          var i = 0
          // TODO: Test BLAS.axpy
          while (i < allStatsSize) {
            allStats(i) += other.allStats(i)
            i += 1
          }
      
          var j = 0
          while (j < statsSize) {
            parentStats(j) += other.parentStats(j)
            j += 1
          }
      
          this
        }
      }
      ```

      该函数将不同partition相同节点的`sufficient statistics`叠加起来，返回调用`DTStatsAggregator`

      其次对于每个节点`nodeIndex`和其对应的汇总`sufficient statistics` `aggStats`，进行真正的分裂尝试，找到最好的切分点

      - 首先找到该节点对应的特征子集`featuresForNode`，该步之前有做过，代码如下：

        ```scala
                val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>
                  Some(nodeToFeatures(nodeIndex))
                }
        ```

      - 寻找该节点对应的最好的切分点

        ```scala
                // find best split for each node
                val (split: Split, stats: ImpurityStats) =
                  binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))
                (nodeIndex, (split, stats))
        ```

        该函数主要尝试分割，计算信息增益，选择最优分割点。具体代码如下：

        ```scala
          private[tree] def binsToBestSplit(
              binAggregates: DTStatsAggregator,
              splits: Array[Array[Split]],
              featuresForNode: Option[Array[Int]],
              node: LearningNode): (Split, ImpurityStats) = {
        
            // Calculate InformationGain and ImpurityStats if current node is top node
            val level = LearningNode.indexToLevel(node.id)
            var gainAndImpurityStats: ImpurityStats = if (level == 0) {
              null
            } else {
              node.stats
            }
        
            val validFeatureSplits =
              Range(0, binAggregates.metadata.numFeaturesPerNode).view.map { featureIndexIdx =>
                featuresForNode.map(features => (featureIndexIdx, features(featureIndexIdx)))
                  .getOrElse((featureIndexIdx, featureIndexIdx))
              }.withFilter { case (_, featureIndex) =>
                binAggregates.metadata.numSplits(featureIndex) != 0
              }
        
            // For each (feature, split), calculate the gain, and select the best (feature, split).
            val splitsAndImpurityInfo =
              validFeatureSplits.map { case (featureIndexIdx, featureIndex) =>
                val numSplits = binAggregates.metadata.numSplits(featureIndex)
                if (binAggregates.metadata.isContinuous(featureIndex)) {
                  // Cumulative sum (scanLeft) of bin statistics.
                  // Afterwards, binAggregates for a bin is the sum of aggregates for
                  // that bin + all preceding bins.
                  val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)
                  var splitIndex = 0
                  while (splitIndex < numSplits) {
                    binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex)
                    splitIndex += 1
                  }
                  // Find best split.
                  val (bestFeatureSplitIndex, bestFeatureGainStats) =
                    Range(0, numSplits).map { case splitIdx =>
                      val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)
                      val rightChildStats =
                        binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)
                      rightChildStats.subtract(leftChildStats)
                      gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,
                        leftChildStats, rightChildStats, binAggregates.metadata)
                      (splitIdx, gainAndImpurityStats)
                    }.maxBy(_._2.gain)
                  (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)
                } else if (binAggregates.metadata.isUnordered(featureIndex)) {
                  // Unordered categorical feature
                  val leftChildOffset = binAggregates.getFeatureOffset(featureIndexIdx)
                  val (bestFeatureSplitIndex, bestFeatureGainStats) =
                    Range(0, numSplits).map { splitIndex =>
                      val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)
                      val rightChildStats = binAggregates.getParentImpurityCalculator()
                        .subtract(leftChildStats)
                      gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,
                        leftChildStats, rightChildStats, binAggregates.metadata)
                      (splitIndex, gainAndImpurityStats)
                    }.maxBy(_._2.gain)
                  (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)
                } else {
                  // Ordered categorical feature
                  val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx)
                  val numCategories = binAggregates.metadata.numBins(featureIndex)
        
                  /* Each bin is one category (feature value).
                   * The bins are ordered based on centroidForCategories, and this ordering determines which
                   * splits are considered.  (With K categories, we consider K - 1 possible splits.)
                   *
                   * centroidForCategories is a list: (category, centroid)
                   */
                  val centroidForCategories = Range(0, numCategories).map { case featureValue =>
                    val categoryStats =
                      binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)
                    val centroid = if (categoryStats.count != 0) {
                      if (binAggregates.metadata.isMulticlass) {
                        // multiclass classification
                        // For categorical variables in multiclass classification,
                        // the bins are ordered by the impurity of their corresponding labels.
                        categoryStats.calculate()
                      } else if (binAggregates.metadata.isClassification) {
                        // binary classification
                        // For categorical variables in binary classification,
                        // the bins are ordered by the count of class 1.
                        categoryStats.stats(1)
                      } else {
                        // regression
                        // For categorical variables in regression and binary classification,
                        // the bins are ordered by the prediction.
                        categoryStats.predict
                      }
                    } else {
                      Double.MaxValue
                    }
                    (featureValue, centroid)
                  }
        
                  logDebug("Centroids for categorical variable: " + centroidForCategories.mkString(","))
        
                  // bins sorted by centroids
                  val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)
        
                  logDebug("Sorted centroids for categorical variable = " +
                    categoriesSortedByCentroid.mkString(","))
        
                  // Cumulative sum (scanLeft) of bin statistics.
                  // Afterwards, binAggregates for a bin is the sum of aggregates for
                  // that bin + all preceding bins.
                  var splitIndex = 0
                  while (splitIndex < numSplits) {
                    val currentCategory = categoriesSortedByCentroid(splitIndex)._1
                    val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1
                    binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)
                    splitIndex += 1
                  }
                  // lastCategory = index of bin with total aggregates for this (node, feature)
                  val lastCategory = categoriesSortedByCentroid.last._1
                  // Find best split.
                  val (bestFeatureSplitIndex, bestFeatureGainStats) =
                    Range(0, numSplits).map { splitIndex =>
                      val featureValue = categoriesSortedByCentroid(splitIndex)._1
                      val leftChildStats =
                        binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)
                      val rightChildStats =
                        binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)
                      rightChildStats.subtract(leftChildStats)
                      gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,
                        leftChildStats, rightChildStats, binAggregates.metadata)
                      (splitIndex, gainAndImpurityStats)
                    }.maxBy(_._2.gain)
                  val categoriesForSplit =
                    categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1)
                  val bestFeatureSplit =
                    new CategoricalSplit(featureIndex, categoriesForSplit.toArray, numCategories)
                  (bestFeatureSplit, bestFeatureGainStats)
                }
              }
        
            val (bestSplit, bestSplitStats) =
              if (splitsAndImpurityInfo.isEmpty) {
                // If no valid splits for features, then this split is invalid,
                // return invalid information gain stats.  Take any split and continue.
                // Splits is empty, so arbitrarily choose to split on any threshold
                val dummyFeatureIndex = featuresForNode.map(_.head).getOrElse(0)
                val parentImpurityCalculator = binAggregates.getParentImpurityCalculator()
                if (binAggregates.metadata.isContinuous(dummyFeatureIndex)) {
                  (new ContinuousSplit(dummyFeatureIndex, 0),
                    ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator))
                } else {
                  val numCategories = binAggregates.metadata.featureArity(dummyFeatureIndex)
                  (new CategoricalSplit(dummyFeatureIndex, Array(), numCategories),
                    ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator))
                }
              } else {
                splitsAndImpurityInfo.maxBy(_._2.gain)
              }
            (bestSplit, bestSplitStats)
          }
        ```

        - 入参分析：

          - `binAggregates`：当前节点对应的统计量，该值已经是汇总版本，只有一个
          - `splits`：所有特征，所有bin对应的`Split`切分点
          - `featuresForNode`：本轮迭代当前节点特征子集
          - `node`：当前节点对象

        - 获取当前节点的不纯度，如果是根节点，那么说明一定是第一次遇到，这个时候该节点未分割时的不纯度置为`null`，否则获取其不纯度，注意，该不纯度也不一定是计算过的，所以也可能为null，后续会判断

          ```scala
              val level = LearningNode.indexToLevel(node.id)
          // level=0，是根节点
              var gainAndImpurityStats: ImpurityStats = if (level == 0) {
                null
              } else {
                node.stats
              }
          ```

        - 获取所有valid的用于分裂的候选特征集合

          ```scala
              val validFeatureSplits =
                Range(0, binAggregates.metadata.numFeaturesPerNode).view.map { featureIndexIdx =>
                    // Option.map 用于 Option[A] => Option[B]，这里将
                    // Option[Array[Int]] => Option[(Int, Int)]
                  featuresForNode.map(features => (featureIndexIdx, features(featureIndexIdx)))
                    .getOrElse((featureIndexIdx, featureIndexIdx))
                }.withFilter { case (_, featureIndex) =>
                    // 过滤掉不可分割的特征
                  binAggregates.metadata.numSplits(featureIndex) != 0
                }
          ```

        - 寻找所有特征的最佳分割点

          ```scala
              val splitsAndImpurityInfo =
                validFeatureSplits.map {...}
          ```

          对于每个特征，执行以下操作

          - 若为连续特征`(binAggregates.metadata.isContinuous(featureIndex))`

            首先，计算累计直方图`cumulative sum of bin statistics`

            ```scala
                      var splitIndex = 0
                      while (splitIndex < numSplits) {
                          // 该函数将splitIndex处的bin和splitIndex+1处的bin统计量叠加后将splitIndex+1保存为该叠加值，通过循环可以得到前面所有bin的累计值
                        binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex)
                        splitIndex += 1
                      }
            ```

            **注意**：连续特征的bin是按照特征值从小往大排的，因此`splitIndex`特征值肯定小于`splitIndex +1`的值

            `mergeForFeature`在`DStatsAggregator`中有定义：

            ```scala
              def mergeForFeature(featureOffset: Int, binIndex: Int, otherBinIndex: Int): Unit = {
                  // 委派给impurityAggregator执行，该函数就是将allStats变量中(featureOffset + otherBinIndex * statsSize)处的值叠加进(featureOffset + binIndex * statsSize)中产生副作用
                impurityAggregator.merge(allStats, featureOffset + binIndex * statsSize,
                  featureOffset + otherBinIndex * statsSize)
              }
            ```

            然后利用该累计直方图计算最佳切割点，执行如下：

            遍历所有切割点：

            ```scala
            Range(0, numSplits).map {...}
            ```

            对于每个切割点，首先获取左集合的不纯度计算器，将用于计算该次分割的不纯度。以为`binAggregates`已经为累计直方图，因此每个bin的值就代表左集合所有bin的累计值

            ```scala
            val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx)
            ```

            这里`DStatsAggregator`委派给`ImpurityAggregator`，而`ImpurityAggregator`会根据具体的实现类找到该实现类对应的`ImpurityCalculator`（为了后续真正的计算），代码如下：

            ```scala
            // DStatsAggregator  
              def getImpurityCalculator(featureOffset: Int, binIndex: Int): ImpurityCalculator = {
                // 委派给ImpurityAggregator
                impurityAggregator.getCalculator(allStats, featureOffset + binIndex * statsSize)
              }
            
            // 不同aggregator的getCalculator实现
            
            // Entropy
              def getCalculator(allStats: Array[Double], offset: Int): EntropyCalculator = {
                new EntropyCalculator(allStats.view(offset, offset + statsSize).toArray)
              }
            
            // Gini
              def getCalculator(allStats: Array[Double], offset: Int): GiniCalculator = {
                new GiniCalculator(allStats.view(offset, offset + statsSize).toArray)
              }
            
            // Variance
              def getCalculator(allStats: Array[Double], offset: Int): VarianceCalculator = {
                new VarianceCalculator(allStats.view(offset, offset + statsSize).toArray)
              }
            ```

            可以所有的`Calculator`获取的是`DStatsAggregator`的`allStats`对应**特征**和**切分点**的`sufficient statistics`，而且是一个`view`。所以可以理解，一个**特征**的一个**切分**对应一个`Calculator`，所以会有$特征数\times 切分点树$个`Calculator`

            对于每个切割点，首先获取右集合的不纯度计算器，将用于计算该次分割的不纯度。这里将采用**总集合值和-左集合值和=右集合值和**的方法加速计算

            ```scala
                          val rightChildStats =
                            binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits)
                          rightChildStats.subtract(leftChildStats)
            ```

            其中，`substract`在`ImpurityCalculator`类中有定义，这是父类即模板类。代码如下：

            ```scala
              def subtract(other: ImpurityCalculator): ImpurityCalculator = {
                var i = 0
                val len = other.stats.length
                while (i < len) {
                  stats(i) -= other.stats(i)
                  i += 1
                }
                this
              }
            ```

            执行的操作就是对应的`sufficient statistics`相减

            既然得到了左集合，右集合的`Calculator`（存储对应特征和切分的`sufficient statistics`），我们计算信息增益

            ```scala
                          gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,
                            leftChildStats, rightChildStats, binAggregates.metadata)
            ```

            计算信息增益的代码如下：

            ```scala
              private def calculateImpurityStats(
                  stats: ImpurityStats,
                  leftImpurityCalculator: ImpurityCalculator,
                  rightImpurityCalculator: ImpurityCalculator,
                  metadata: DecisionTreeMetadata): ImpurityStats = {
            
                  // 首先判断当前节点的不纯度是否已经计算，若未计算，那么将左右集合的统计值叠加可以获得整个节点的统计值
                val parentImpurityCalculator: ImpurityCalculator = if (stats == null) {
                  leftImpurityCalculator.copy.add(rightImpurityCalculator)
                } else {
                  stats.impurityCalculator
                }
            
                  // 调用calculate方法得到整个节点的不纯度
                val impurity: Double = if (stats == null) {
                  parentImpurityCalculator.calculate()
                } else {
                  stats.impurity
                }
            
                  // 左右集合以及整个节点的样本数量
                val leftCount = leftImpurityCalculator.count
                val rightCount = rightImpurityCalculator.count
            
                val totalCount = leftCount + rightCount
            
                // If left child or right child doesn't satisfy minimum instances per node,
                // then this split is invalid, return invalid information gain stats.
                  // 若分裂，得到的节点数量太小，小于设定阈值，因此停止分裂，返回一个标志为Invalid的ImpurityStats
                if ((leftCount < metadata.minInstancesPerNode) ||
                  (rightCount < metadata.minInstancesPerNode)) {
                  return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)
                }
            
                  // 分别计算左右集合的不纯度
                val leftImpurity = leftImpurityCalculator.calculate() // Note: This equals 0 if count = 0
                val rightImpurity = rightImpurityCalculator.calculate()
            
                  
                val leftWeight = leftCount / totalCount.toDouble
                val rightWeight = rightCount / totalCount.toDouble
            
                  // 总的不纯度要乘以样本量权重，最后得到增益
                val gain = impurity - leftWeight * leftImpurity - rightWeight * rightImpurity
            
                // if information gain doesn't satisfy minimum information gain,
                // then this split is invalid, return invalid information gain stats.
                  // 增益不够，不分裂，同样返回Invalid占位对象
                if (gain < metadata.minInfoGain) {
                  return ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator)
                }
            
                  // 如果一切顺利，返回ImpurityStats，里面包含了该节点本次分裂所产生的各个计算值，包括：增益，整个节点不纯度，整个节点不纯度计算器，
                new ImpurityStats(gain, impurity, parentImpurityCalculator,
                  leftImpurityCalculator, rightImpurityCalculator)
              }
            ```

            该段代码有以下几点需要注意：

            - `add`：和`ImpurityCalculator`的`substract`函数一样

            - 返回参数`ImpurityStats`

              ```scala
              private[spark] class ImpurityStats(
                  val gain: Double,
                  val impurity: Double,
                  val impurityCalculator: ImpurityCalculator,
                  val leftImpurityCalculator: ImpurityCalculator,
                  val rightImpurityCalculator: ImpurityCalculator,
                  val valid: Boolean = true) extends Serializable
              ```

              该参数汇总了**某节点**，**某特征**，**某分割下**的不纯度计算相关统计值。包括：分割会产生的信息增益及左右集合的`Calculator`，若不分割整个节点的不纯度及其`Calculator`，是否可以分割`valid`标记

            - `ImpurityCalculator.calculate`方法

              该方法是`Calculator`最核心的用处，该方法会交给实现类具体实现，三种实现类的实现方法分别为：

              ```scala
              // Entropy
                def calculate(): Double = Entropy.calculate(stats, stats.sum)
              // Gini
                def calculate(): Double = Gini.calculate(stats, stats.sum)
              // Variance
                def calculate(): Double = Variance.calculate(stats(0), stats(1), stats(2))
              ```

              三种实现类分别交给三种衡量指标的静态类去实现，传入的参数就是一个节点一个特征一次分割计算不纯度所需要的`sufficient statistics`，具体的计算公式如下：

              ```scala
              // Entropy
                override def calculate(counts: Array[Double], totalCount: Double): Double = {
                  if (totalCount == 0) {
                    return 0
                  }
                  val numClasses = counts.length
                  var impurity = 0.0
                  var classIndex = 0
                  while (classIndex < numClasses) {
                    val classCount = counts(classIndex)
                    if (classCount != 0) {
                      val freq = classCount / totalCount
                      impurity -= freq * log2(freq)
                    }
                    classIndex += 1
                  }
                  impurity
                }
              
              // Gini
                override def calculate(counts: Array[Double], totalCount: Double): Double = {
                  if (totalCount == 0) {
                    return 0
                  }
                  val numClasses = counts.length
                  var impurity = 1.0
                  var classIndex = 0
                  while (classIndex < numClasses) {
                    val freq = counts(classIndex) / totalCount
                    impurity -= freq * freq
                    classIndex += 1
                  }
                  impurity
                }
              
              // Variance
                override def calculate(count: Double, sum: Double, sumSquares: Double): Double = {
                  if (count == 0) {
                    return 0
                  }
                  val squaredLoss = sumSquares - (sum * sum) / count
                  squaredLoss / count
                }
              ```

              `Entropy`和`Gini`的计算比较容易理解，根据定义的公式进行计算，`Variance`的计算来自于以下公式推导：

              $$\begin{equation}\begin{split}\frac{1}{N}\sum_{i=1}^{N}(y_{i}-\frac{\sum_{i=1}^{N}y_{i}}{N})^{2} &= \frac{1}{N}\sum_{i=1}^{N}(y_{i}^{2}-2\frac{y_{i}\sum_{i=1}^{N}y_{i}}{N}+\frac{(\sum_{i=1}^{N}y_{i})^{2}}{N^{2}})\\&=\frac{1}{N}(\sum_{i=1}^{N}y_{i}^{2}-2\frac{(\sum_{i=1}^{N}y_{i})^{2}}{N}+\frac{(\sum_{i=1}^{N}y_{i})^{2}}{N})\\&=\frac{1}{N}(\sum_{i=1}^{N}y_{i}^{2}-\frac{(\sum_{i=1}^{N}y_i)^{2}}{N})\end{split}\end{equation}$$

              均方差转换成右边形式，知道在`VarianceAggregator`中收集的三个`sufficient statistics` `count`、`sum`以及`sumSquares`就是对应的右边公式的$N$，$\sum_{i=1}^{N}y_{i}$以及$\sum_{i=1}^{N}y_{i}^{2}$

            - `ImpurityCalculator.count`统计`aggregate`的样本量（对于有放回采样不是特别确切，可能有些样本重复采样）

              该函数对应于不同的`aggregator`也有不同的实现

              ```scala
              // Entropy
                def count: Long = stats.sum.toLong
              // Gini
                def count: Long = stats.sum.toLong
              // Variance
                def count: Long = stats(0).toLong
              ```

            - `ImpurityStats.getInvalidImpurityStats`：无法分割节点的不纯度统计占位值

              ```scala
                def getInvalidImpurityStats(impurityCalculator: ImpurityCalculator): ImpurityStats = {
                  new ImpurityStats(Double.MinValue, impurityCalculator.calculate(),
                    impurityCalculator, null, null, false)
                }
              ```

            最后返回该节点本次切割所对应的不纯度和信息增益值

            ```scala
            (splitIdx, gainAndImpurityStats)
            ```

            找到最大的信息增益值以及切割索引

            ```scala
            val (bestFeatureSplitIndex, bestFeatureGainStats) = Range(0, numSplits).map {...}.maxBy(_._2.gain)
            ```

            将每个切分索引转换成切分值`Split`返回，保存为`Array[(feature, bestSplit)]`这样的对象

            ```scala
            val splitsAndImpurityInfo = validFeatureSplits.map {
                ...
                (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats)
            }
            ```

          - 若为无序类别特征：`binAggregates.metadata.isUnordered(featureIndex)`

            首先计算当前特征对应的`allStats`中的索引位置

            ```scala
            val leftChildOffset = binAggregates.getFeatureOffset(featureIndexIdx)
            ```

            然后对当前特征的每一个切分进行操作，从而找到最好的切分

            ```scala
                      val (bestFeatureSplitIndex, bestFeatureGainStats) =
                        Range(0, numSplits).map { splitIndex => ...
                      }.maxBy(_._2.gain)
            ```

            对于无序类别特征，每个`bin`都是一种切分`split`方法，这点和连续有序特征完全不同，且每个`bin`保存的是左集合的统计量。因此计算每种切分的信息增益，只需要针对该切分对应的`bin`进行计算即可，而不用像有序特征那样先计算累计直方图

            左集合的`Calculator`

            ```scala
            val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex)
            ```

            右集合通过总的相减左集合得到

            ```scala
                          val rightChildStats = binAggregates.getParentImpurityCalculator()
                            .subtract(leftChildStats)
            ```

            下面计算信息增益等方式和连续的一致

            ```scala
                          gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,
                            leftChildStats, rightChildStats, binAggregates.metadata)
                          (splitIndex, gainAndImpurityStats)
            ```

          - 若为有序类别变量

            有序类别变量比较特殊，既有连续特征的大小关系，也有无序类别特征的离散特性。因为类别之间没有连续值过渡，所以和离散的连续值不同，在这里统计累计直方图时，采用的是根据不纯度（分类）或标签平均值（回归）进行排序累计，是一种目标导向型的切分方式。事实上我认为其实有序类别特征也可以采用无序类别变量的切分存储和寻优方式，只是类别多了后计算量太大因此并未这样采用。所以采用了效果相对次优的根据每个bin的好坏启发性的划分，所以K个类别将会有K-1个划分方法。为啥是启发性的呢？因为两个bin都好并不代表合在一起就好，因为可能两个bin包含完全不同的类别，类别越多，这种启发式寻优的效果会越差。那为何连续变量不采用这种方式呢，即分段式的切分。个人认为没有必要，因为在树变深的过程中是有可能重复切分某个特征的，这个时候就会产生分段切分的效果。

            对于有序类别变量，每个bin是一个category，首先针对每个category计算其`centroid`。`centroid`上面有讨论，代表bin内的不纯度

            ```scala
                      val centroidForCategories = Range(0, numCategories).map { case featureValue =>
                          // 对于每个bin，或者category
                          // 首先获取其统计量
                        val categoryStats =
                          binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)
                          // 其centroid值不同的分类任务计算方法不同
                        val centroid = if (categoryStats.count != 0) {
                          if (binAggregates.metadata.isMulticlass) {
            				// 多分类该bin的不纯度作为centroid
                            categoryStats.calculate()
                          } else if (binAggregates.metadata.isClassification) {
            				// 二分类需要2个统计值，1索引处表示类别为1的统计量
                            categoryStats.stats(1)
                          } else {
            				// 回归取其预测值，实际上是取当前特征是该特征的样本的label值的平均，后面会讨论其计算
                            categoryStats.predict
                          }
                        } else {
                          Double.MaxValue
                        }
                          // 返回该bin值，或category值及其对应的centroid值
                        (featureValue, centroid)
                      }
            ```

            `ImpurityCalculator.predict`针对三种不纯度有三种实现方法（其实为两种，该方法和分类还是回归有关）

            ```scala
            // EntropyCalculator
              def predict: Double = if (count == 0) {
                0
              } else {
                indexOfLargestArrayElement(stats)
              }
            
              protected def indexOfLargestArrayElement(array: Array[Double]): Int = {
                  // 利用currentValue保留当前索引的技巧
                  // 在array中找到元素最大（即数量最多）的索引作为该类（array建立的时候就是根据索引作为类别来建的）
                val result = array.foldLeft(-1, Double.MinValue, 0) {
                  case ((maxIndex, maxValue, currentIndex), currentValue) =>
                    if (currentValue > maxValue) {
                      (currentIndex, currentValue, currentIndex + 1)
                    } else {
                      (maxIndex, maxValue, currentIndex + 1)
                    }
                }
                result._1
              }
            
            // GiniCalculator
            // 和EntropyCalculator一致
            
            // VarianceCalculator
              def predict: Double = if (count == 0) {
                0
              } else {
                  
                stats(1) / count
              }
            ```

            然后根据`centroid`对bin进行排序，得到排序后的`Array[(featureValue, centroid)]`

            ```scala
                      val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2)
            ```

            根据`centroid`大小构建累计直方图

            ```scala
                      var splitIndex = 0
                      while (splitIndex < numSplits) {
                        val currentCategory = categoriesSortedByCentroid(splitIndex)._1
                        val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1
                          // 按照centroid从小到大的顺序累计直方图
                        binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory)
                        splitIndex += 1
                      }
            ```

            获取`centroid`最大的类别，该索引在计算该节点总的不纯度时会用到

            ```scala
                      val lastCategory = categoriesSortedByCentroid.last._1
            ```

            下面的遍历所有切分点，找到最好的切分点和连续变量类似，只是顺序是按照`centroid`的顺序

            ```scala
                      val (bestFeatureSplitIndex, bestFeatureGainStats) =
                        Range(0, numSplits).map { splitIndex =>
                          val featureValue = categoriesSortedByCentroid(splitIndex)._1
                            // 左统计量
                          val leftChildStats =
                            binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue)
                            // 总统计量
                          val rightChildStats =
                            binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory)
                            // 右统计量
                          rightChildStats.subtract(leftChildStats)
                          gainAndImpurityStats = calculateImpurityStats(gainAndImpurityStats,
                            leftChildStats, rightChildStats, binAggregates.metadata)
                          (splitIndex, gainAndImpurityStats)
                        }.maxBy(_._2.gain)
            ```

            找到最好的切分点后，将切分点左边的类别都找出来，该步非常重要，因为找到所有的左集合类别后，后面的处理就和无序类别一样了。很显然，这个左边是按照`centroid`进行衡量的左边

            ```
                      val categoriesForSplit =
                        categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1)
            ```

            构建类别切分并返回

            ```scala
                      val bestFeatureSplit =
                        new CategoricalSplit(featureIndex, categoriesForSplit.toArray, numCategories)
                      (bestFeatureSplit, bestFeatureGainStats)
            ```

          至此，当前节点对应的最佳切分点`Split`及其对应的`ImpurityStats`都找到了

        - 如果寻找最优切分点过程中发现不能切分，对这种情况做一些处理

          ```scala
              val (bestSplit, bestSplitStats) =
                  if (splitsAndImpurityInfo.isEmpty) {
                  val dummyFeatureIndex = featuresForNode.map(_.head).getOrElse(0)
                  val parentImpurityCalculator = binAggregates.getParentImpurityCalculator()
                  if (binAggregates.metadata.isContinuous(dummyFeatureIndex)) {
                      // 连续变量返回ContinousSplit的dummy值，占位作用，threshold设为0
                    (new ContinuousSplit(dummyFeatureIndex, 0),
                      ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator))
                  } else {
                      // 类别变量返回CategoricalSplit的dummy值，占位注意，左类别数组为空，这种不会处理
                    val numCategories = binAggregates.metadata.featureArity(dummyFeatureIndex)
                    (new CategoricalSplit(dummyFeatureIndex, Array(), numCategories),
                      ImpurityStats.getInvalidImpurityStats(parentImpurityCalculator))
                  }
                } else {
                    // 正常返回
                  splitsAndImpurityInfo.maxBy(_._2.gain)
                }
          ```

        - 最终返回

          ```
              (bestSplit, bestSplitStats)
          ```

        至此，寻找最优切分点`binToBestSplit`已经结束，返回当前节点`nodeIndex`对应的最优切分点及其统计信息`(split: Split, stats: ImpurityStats)`

      - 这样`nodeToBestSplit`就得到所有节点对应的最优切分点及其统计信息了

        ```scala
            val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) => a.merge(b)).map {
              case (nodeIndex, aggStats) =>
                val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =>
                  Some(nodeToFeatures(nodeIndex))
                }
        
                // find best split for each node
                val (split: Split, stats: ImpurityStats) =
                  binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex))
                (nodeIndex, (split, stats))
            }.collectAsMap()
        ```

        最后的`collectAsMap`将最终的计算值保存到`driver`端

      - 下面开始建树（以下都是在`driver`端进行）

        - 对于每棵树及该棵树下的所有本轮需要迭代的节点

          ```scala
              // Iterate over all nodes in this group.
              nodesForGroup.foreach { case (treeIndex, nodesForTree) =>
                nodesForTree.foreach { node =>
                  ...
                }
              }
          ```

        - 获取当前节点最优切分信息

          ```scala
                  val nodeIndex = node.id
                  val nodeInfo = treeToNodeToIndexInfo(treeIndex)(nodeIndex)
                  val aggNodeIndex = nodeInfo.nodeIndexInGroup
                  val (split: Split, stats: ImpurityStats) =
                    nodeToBestSplits(aggNodeIndex)
          ```

        - 确定该节点是否满足为叶子节点的条件：信息增益不够或者超过预设的树最大深度

          ```scala
                  // Extract info for this node.  Create children if not leaf.
                  val isLeaf =
                    (stats.gain <= 0) || (LearningNode.indexToLevel(nodeIndex) == metadata.maxDepth)
                  node.isLeaf = isLeaf
                  node.stats = stats
          ```

          后面会判断，如果不能分裂那么就什么也不做，该节点的分裂结束

        - 如果不是叶子节点，将进行分裂

          首先将计算得到的信息增益相关统计信息保存到节点变量里，包括：节点切分信息，左右孩子是否为叶子等信息

          ```scala
                    node.split = Some(split)
                    val childIsLeaf = (LearningNode.indexToLevel(nodeIndex) + 1) == metadata.maxDepth
                    val leftChildIsLeaf = childIsLeaf || (stats.leftImpurity == 0.0)
                    val rightChildIsLeaf = childIsLeaf || (stats.rightImpurity == 0.0)
                    node.leftChild = Some(LearningNode(LearningNode.leftChildIndex(nodeIndex),
                      leftChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.leftImpurityCalculator)))
                    node.rightChild = Some(LearningNode(LearningNode.rightChildIndex(nodeIndex),
                      rightChildIsLeaf, ImpurityStats.getEmptyImpurityStats(stats.rightImpurityCalculator)))
          ```

        - 递归建树

          ```scala
                    // enqueue left child and right child if they are not leaves
                    if (!leftChildIsLeaf) {
                      nodeStack.push((treeIndex, node.leftChild.get))
                    }
                    if (!rightChildIsLeaf) {
                      nodeStack.push((treeIndex, node.rightChild.get))
                    }
          ```

          所有最核心的建树过程就这么两行，外面的大循环会对`nodeStack`判空结束。使用栈的好处是可以一颗一颗的建，而非同时建多棵树，减少网络传输

- 保存模型

  ```scala
          if (strategy.algo == OldAlgo.Classification) {
            topNodes.map { rootNode =>
              new DecisionTreeClassificationModel(rootNode.toNode, numFeatures,
                strategy.getNumClasses)
            }
          } else {
            topNodes.map(rootNode => new DecisionTreeRegressionModel(rootNode.toNode, numFeatures))
          }
  ```

  最终返回的是`Array of DecisionTree`，而`DecisionTree`其实保存的主要是根节点。保存决策树的过程中，之前的`LearningNode`会转成`Node`

- 保存为随机森林模型

  ```scala
    def run(input: RDD[LabeledPoint]): RandomForestModel = {
      val trees: Array[NewDTModel] = NewRandomForest.run(input.map(_.asML), strategy, numTrees,
        featureSubsetStrategy, seed.toLong, None)
      new RandomForestModel(strategy.algo, trees.map(_.toOld))
    }
  ```

  这里通过`Array of DecisionTree`模型和任务类型初始化随机森林模型

  至此训练基本完成

- 预测

  ```scala
      val labelAndPreds = testData.map { point =>
        val prediction = model.predict(point.features)
        (point.label, prediction)
      }
  ```

  针对每个点的特征，用训练好的模型进行预测。这里的特征不是训练时bin化的值

  具体执行逻辑如下：

  ```scala
    def predict(features: Vector): Double = {
      (algo, combiningStrategy) match {
        case (Regression, Sum) =>
          predictBySumming(features)
        case (Regression, Average) =>
          predictBySumming(features) / sumWeights
        case (Classification, Sum) => // binary classification
          val prediction = predictBySumming(features)
          // TODO: predicted labels are +1 or -1 for GBT. Need a better way to store this info.
          if (prediction > 0.0) 1.0 else 0.0
        case (Classification, Vote) =>
          predictByVoting(features)
        case _ =>
          throw new IllegalArgumentException(
            "TreeEnsembleModel given unsupported (algo, combiningStrategy) combination: " +
              s"($algo, $combiningStrategy).")
      }
    }
  ```

  该方法是`TreeEnsembleModels`的方法，`TreeEnsembleModel`是模板类，里面有个域为`combiningStrategy`即结果的融合策略。该类有两个具体的实现，分别为`RandomForestModel`和`GradientBoostedTreesModel`，代码如下：

  ```scala
  class GradientBoostedTreesModel @Since("1.2.0") (
      @Since("1.2.0") override val algo: Algo,
      @Since("1.2.0") override val trees: Array[DecisionTreeModel],
      @Since("1.2.0") override val treeWeights: Array[Double])
    extends TreeEnsembleModel(algo, trees, treeWeights, combiningStrategy = Sum)
  
  class RandomForestModel @Since("1.2.0") (
      @Since("1.2.0") override val algo: Algo,
      @Since("1.2.0") override val trees: Array[DecisionTreeModel])
    extends TreeEnsembleModel(algo, trees, Array.fill(trees.length)(1.0),
      combiningStrategy = if (algo == Classification) Vote else Average)
  ```

  可以看到模型会初始化超类的`combiningStrategy`变量。可以看到，随机森林分类时的融合策略为投票，而回归时为平均。梯度树的融合策略为求和

  所有随机森林会调用以下两个方法

  ```scala
        case (Classification, Vote) => predictByVoting(features)
  	// 随机森林的所有树的sumWeights，既然会设置每颗树的权重那说明梯度树每棵树的权重是不一样的
        case (Regression, Average) => predictBySumming(features) / sumWeights
  ```

  这两个方法就是递归找最终所在的节点，并返回该节点的预测值。

## object ml.tree.impl.DecisionTreeMetaData

- 通过`buildMetadata`将之前设置的配置参数转化为实际的作用。主要有两个作用：
  - 确定哪些特征有序，哪些特征无序
  - 确定每个特征的splits和bins的个数

- GBDT也采用该类完成上述两个计算

- 执行逻辑

  通过静态类调用

  ```scala
  val metadata = DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)
  ```

  进入主逻辑

  ```scala
    def buildMetadata(
        input: RDD[LabeledPoint],
        strategy: Strategy,
        numTrees: Int,
        featureSubsetStrategy: String): DecisionTreeMetadata = {
  
      val numFeatures = input.map(_.features.size).take(1).headOption.getOrElse {
        throw new IllegalArgumentException(s"DecisionTree requires size of input RDD > 0, " +
          s"but was given by empty one.")
      }
      require(numFeatures > 0, s"DecisionTree requires number of features > 0, " +
        s"but was given an empty features vector")
      val numExamples = input.count()
      val numClasses = strategy.algo match {
        case Classification => strategy.numClasses
        case Regression => 0
      }
  
      val maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt
      if (maxPossibleBins < strategy.maxBins) {
        logWarning(s"DecisionTree reducing maxBins from ${strategy.maxBins} to $maxPossibleBins" +
          s" (= number of training instances)")
      }
  
      // We check the number of bins here against maxPossibleBins.
      // This needs to be checked here instead of in Strategy since maxPossibleBins can be modified
      // based on the number of training examples.
      if (strategy.categoricalFeaturesInfo.nonEmpty) {
        val maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max
        val maxCategory =
          strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1
        require(maxCategoriesPerFeature <= maxPossibleBins,
          s"DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the " +
          s"number of values in each categorical feature, but categorical feature $maxCategory " +
          s"has $maxCategoriesPerFeature values. Considering remove this and other categorical " +
          "features with a large number of values, or add more training examples.")
      }
  
      val unorderedFeatures = new mutable.HashSet[Int]()
      val numBins = Array.fill[Int](numFeatures)(maxPossibleBins)
      if (numClasses > 2) {
        // Multiclass classification
        val maxCategoriesForUnorderedFeature =
          ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt
        strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>
          // Hack: If a categorical feature has only 1 category, we treat it as continuous.
          // TODO(SPARK-9957): Handle this properly by filtering out those features.
          if (numCategories > 1) {
            // Decide if some categorical features should be treated as unordered features,
            //  which require 2 * ((1 << numCategories - 1) - 1) bins.
            // We do this check with log values to prevent overflows in case numCategories is large.
            // The next check is equivalent to: 2 * ((1 << numCategories - 1) - 1) <= maxBins
            if (numCategories <= maxCategoriesForUnorderedFeature) {
              unorderedFeatures.add(featureIndex)
              numBins(featureIndex) = numUnorderedBins(numCategories)
            } else {
              numBins(featureIndex) = numCategories
            }
          }
        }
      } else {
        // Binary classification or regression
        strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>
          // If a categorical feature has only 1 category, we treat it as continuous: SPARK-9957
          if (numCategories > 1) {
            numBins(featureIndex) = numCategories
          }
        }
      }
  
      // Set number of features to use per node (for random forests).
      val _featureSubsetStrategy = featureSubsetStrategy match {
        case "auto" =>
          if (numTrees == 1) {
            "all"
          } else {
            if (strategy.algo == Classification) {
              "sqrt"
            } else {
              "onethird"
            }
          }
        case _ => featureSubsetStrategy
      }
  
      val numFeaturesPerNode: Int = _featureSubsetStrategy match {
        case "all" => numFeatures
        case "sqrt" => math.sqrt(numFeatures).ceil.toInt
        case "log2" => math.max(1, (math.log(numFeatures) / math.log(2)).ceil.toInt)
        case "onethird" => (numFeatures / 3.0).ceil.toInt
        case _ =>
          Try(_featureSubsetStrategy.toInt).filter(_ > 0).toOption match {
            case Some(value) => math.min(value, numFeatures)
            case None =>
              Try(_featureSubsetStrategy.toDouble).filter(_ > 0).filter(_ <= 1.0).toOption match {
                case Some(value) => math.ceil(value * numFeatures).toInt
                case _ => throw new IllegalArgumentException(s"Supported values:" +
                  s" ${RandomForestParams.supportedFeatureSubsetStrategies.mkString(", ")}," +
                  s" (0.0-1.0], [1-n].")
              }
          }
      }
  
      new DecisionTreeMetadata(numFeatures, numExamples, numClasses, numBins.max,
        strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins,
        strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth,
        strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode)
    }
  ```

  确定以下几个内容：

  - `numFeatures`：特征数量。选取一行计算列数获取

  - `numExamples`：样本数。`RDD`的`count`函数获取

  - `numClasses`：类别数。分类任务为`strategy.numClasses`，回归任务忽略，取个占位值0

  - `maxPossibleBins`：取配置参数`strategy.maxBins`和`numExamples`的最小值。如果`numExamples`比`maxPossibleBins`还小，会报warning。说明样本量太少。这里还需要check一下`maxPossibleBins`必须大于`maxCategoriesPerFeature`（所有类别特征的类别数最大值）。否则会损失部分类别特征的信息。

  - `numBins`：每个类别特征的分箱数。Spark的实现中将连续变量的类别数置为1。

    - 若为二分类或者回归

      如果一个类别特征的类别只能取一种值，将该特征的bin数置为1，代表连续变量；

      否则为`strategy.categoricalFeatureInfo`中记录的bin数量

    - 若为多分类

      多分类情况下，某些特征的类别数如果小于`maxPossibleBins`能够允许的类别数，会采用无序类别特征的方式对待

      ```scala
      // Multiclass classification
      val maxCategoriesForUnorderedFeature =
      ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt
      strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>
          // Hack: If a categorical feature has only 1 category, we treat it as continuous.
          // TODO(SPARK-9957): Handle this properly by filtering out those features.
          if (numCategories > 1) {
              // Decide if some categorical features should be treated as unordered features,
              //  which require 2 * ((1 << numCategories - 1) - 1) bins.
              // We do this check with log values to prevent overflows in case numCategories is large.
              // The next check is equivalent to: 2 * ((1 << numCategories - 1) - 1) <= maxBins
              if (numCategories <= maxCategoriesForUnorderedFeature) {
                  unorderedFeatures.add(featureIndex)
                  numBins(featureIndex) = numUnorderedBins(numCategories)
              } else {
                  numBins(featureIndex) = numCategories
              }
          }
      }
      ```

      若一个特征的类别数为`arity`个，那么该特征能够组合的不同splits数量为：$2^{arity - 1} - 1$，这是通过排列组合得到的（split后的两个集合为非空）。具体的代码实现为：

      ```scala
      def numUnorderedBins(arity: Int): Int = (1 << arity - 1) - 1
      ```

      因为可以有 $2^{arity-1}-1$ 种不同的切割法，所以可以产生 $2 (2^{arity-1}-1)$ 种不同的bins，左右各一半。在实现时，为了防止类别数可能太大导致的指数溢出，判断时采用对数计算。

      如果类别数大于`maxPossibleBins`能够允许的类别数，则采用原来的类别数作为bin的数量

  - `_featureSubsetStrategy`：特征的子集选择策略（只有随机森林用到，传统的GBDT不使用）。主要确定在参数值为`auto`时的指定方式。若`numTrees`为1，则为`all`，即所有的特征都会使用；否则，分类任务采用`sqrt`，回归任务采用`onethird`。

  - `numFeaturesPerNode`：根据特征子集的选择策略，计算出在每次节点分类时，需要保留多少特征数。

    - `all`：保留全部特征
    - `sqrt`：保留开方后个量的特征
    - `log2`：保留取对数后个量的特征
    - `onethird`：保留$\frac{1}{3}$个量的特征
    - 整数：保留该整数个量的特征
    - `[0,1]`分数：保留该比例个量的特征

- 最后将这些计算好的参数保存进`DecisionTreeMetaData`对象里，后面的计算使用的参数都会通过该对象获取。