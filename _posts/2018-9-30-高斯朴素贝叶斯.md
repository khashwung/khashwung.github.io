---
layout: post
title: "sklearn的Gaussian Naive Bayesian实现"
author: "kashwung"
categories: journal
tags: [documentation,sample]
image: cutting.jpg
---

# 组件GaussianNB

## 原理

- 概率意义下的分类：预测特征为$X=\left\{X_{1}=x_{1},X_{2}=x_{2},...,X_{k}=x_{k}\right\}$下各个类的概率。通过找到啊概率最大的类用于分类任务。$k$为特征数。所以重点是如何计算条件概率$P(Y=y_{i}|X=x)$。可以看到这是判别模型

- 朴素的假设：各个特征直接互相独立，因此将各个特征拆解开来变成特征连乘的形式

  $$\begin{equation}\begin{split}P(Y=y_{i}|X=x)&=P(Y=y_{i}|X_{1}=x_{1},...,X_{k}=x_{k})\\&=P(Y=y_{i}|X_{1}=x_{1})\times...\times P(Y=y_{i}|X_{k}=x_{k})\\&=\prod_{j=1}^{k}P(Y=y_{i}|X_{j}=x_{j})\end{split}\end{equation}$$

- 贝叶斯公式：后验=似然*先验

  $$\begin{equation}\begin{split}P(Y=y_{i}|X=x)&=\prod_{j=1}^{k}P(Y=y_{i}|X_{j}=x_{j})\\&=\prod_{j=1}^{k}\frac{P(X_{j}=x_{j}|Y=y_{i})P(Y=y_{j})}{P(X_{j}=x_{j})}\\&=\prod_{j=1}^{k}\frac{P(X_{j}=x_{j}|Y=y_{i})P(Y=y_{j})}{\sum_{i=1}^{c}P(X_{j}=x_{j}|Y=y_{i})P(Y=y_{j})}\\&=\frac{\prod_{j=1}^{k}P(X_{j}=x_{j}|Y=y_{i})P(Y=y_{j})}{\prod_{j=1}^{k}\sum_{i=1}^{c}P(X_{j}=x_{j}|Y=y_{i})P(Y=y_{j})}\end{split}\end{equation}$$

- 对数概率与归一化因子：在真正实现时，如果不需要计算每个条件概率而是找出概率最大的类作为类标签，那么可以省去分母的计算，因为分母对于不同的类别而言都是常量。同时在数值计算的实现中，为了防止数值溢出通常会采用对数概率。

  $$\begin{equation}\begin{split}logP(Y=y_{i}|X=x)&\propto log\prod_{j=1}^{k}P(X_{j}=x_{j}|Y=y_{i})P(Y=y_{j})\\&= \sum_{j=1}^{k}(logP(X_{j}=x_{j}|Y=y_{i})+logP(Y=y_{j}))\end{split}\end{equation}$$

- 特征的分布：特征分为离散变量和连续变量，不同类型的变量有不同的分布，且概率是以概率密度还是概率的表达形式不同。在本文所探究的GNB模型中，其中Gaussian就是假设：**所有的特征是连续值且服从高斯分布**。当然还有其它的特征及分布形式，比如伯努利分布（特征值只有两个取值，表达是否信息），多项式分布（特征可以取多个离散值，所有样本（多次试验，一个样本一次试验）统计完后得到的样本分布）等。在这些分布中，给定一个特征值和一个类，即可以得到**该特征值该类**对应的概率值或者概率密度

  第$i$类第$j$个特征的对数高斯概率表达如下：

  $$\begin{equation}\begin{split}logP(X_{j}=x_{j}|Y=y_{i})&=log(\frac{1}{\sqrt{2\pi}\sigma_{ij}}exp(-\frac{(x-\mu_{ij})^{2}}{2\sigma_{ij}^{2}}))\\&=-0.5log(2\pi\sigma_{ij}^{2})-\frac{(x-\mu_{ij})^{2}}{2\sigma_{ij}^{2}}\end{split}\end{equation}$$

  代入上式沿着特征$j$维度累加最终得到计算值

  可以看到，该生成模型的参数主要是以下三个：每个类别每个特征对应的$\sigma_{ij}^{2}$；每个特征每个类别对应的$\mu_{ij}$；类别的先验$P(Y=y_{j})$。

  最终总的计算公式如下：

  $$\begin{equation}\begin{split}logP(Y=y_{i}|X=x)&\propto \sum_{j=1}^{k}(logP(X_{j}=x_{j}|Y=y_{i})+logP(Y=y_{j}))\\&=\sum_{j=1}^{k}(-0.5log(2\pi\sigma_{ij}^{2}))+\sum_{j=1}^{k}(-\frac{(x-\mu_{ij}^{2})^{2}}{2\sigma_{ij}^{2}})+\sum_{j=1}^{k}(logP(Y=y_{j}))\end{split}\end{equation}$$

- 训练过程：如何得到模型的三个参数值

  

  

## 训练

- `fit`方法

  ```python
  def fit(self, X, y, sample_weight=None):
      X, y = check_X_y(X, y)
      return self._partial_fit(X, y, np.unique(y), _refit=True,
                               sample_weight=sample_weight)
  
      def _partial_fit(self, X, y, classes=None, _refit=False,
                       sample_weight=None):
  		# 验证数据格式
          X, y = check_X_y(X, y)
          if sample_weight is not None:
              sample_weight = check_array(sample_weight, ensure_2d=False)
              check_consistent_length(y, sample_weight)
  
          # 保证方差一定大于该值，防止方差作为分母时的数据溢出
          epsilon = 1e-9 * np.var(X, axis=0).max()
  
          if _refit:
              self.classes_ = None
  
          if _check_partial_fit_first_call(self, classes):
              # 主要进入该分支
              # meta参数
              n_features = X.shape[1]
              n_classes = len(self.classes_)
              
              # 以下两个变量就是我们之前分析的GNB的对于G的两个参数，训练模型就是得到这两个参数
              self.theta_ = np.zeros((n_classes, n_features))
              self.sigma_ = np.zeros((n_classes, n_features))
  
              self.class_count_ = np.zeros(n_classes, dtype=np.float64)
  
              # Initialise the class prior
              n_classes = len(self.classes_)
              # Take into account the priors
              if self.priors is not None:
                  priors = np.asarray(self.priors)
                  # Check that the provide prior match the number of classes
                  if len(priors) != n_classes:
                      raise ValueError('Number of priors must match number of'
                                       ' classes.')
                  # Check that the sum is 1
                  if priors.sum() != 1.0:
                      raise ValueError('The sum of the priors should be 1.')
                  # Check that the prior are non-negative
                  if (priors < 0).any():
                      raise ValueError('Priors must be non-negative.')
                  self.class_prior_ = priors
              else:
                  # GNB模型的第三个参数，类先验参数
                  self.class_prior_ = np.zeros(len(self.classes_),
                                               dtype=np.float64)
          else:
              if X.shape[1] != self.theta_.shape[1]:
                  msg = "Number of features %d does not match previous data %d."
                  raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))
              # Put epsilon back in each time
              self.sigma_[:, :] -= epsilon
  
          classes = self.classes_
  
          unique_y = np.unique(y)
          unique_y_in_classes = np.in1d(unique_y, classes)
  
          if not np.all(unique_y_in_classes):
              raise ValueError("The target label(s) %s in y do not exist in the "
                               "initial classes %s" %
                               (unique_y[~unique_y_in_classes], classes))
  		# 所有的类别
          for y_i in unique_y:
              # 该段是具体的收集过程
              # 类y_i在classes中的索引
              i = classes.searchsorted(y_i)
              # 过滤出所有样本标签为y_i的样本
              X_i = X[y == y_i, :]
  
              if sample_weight is not None:
                  # 如果设置采样权重，那么统计采样权重
                  sw_i = sample_weight[y == y_i]
                  N_i = sw_i.sum()
              else:
                  sw_i = None
                  N_i = X_i.shape[0]
  
              # 计算平均数和方差。该实现考虑了增量式的更新
              new_theta, new_sigma = self._update_mean_variance(
                  self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],
                  X_i, sw_i)
  
              # 更新并保持
              self.theta_[i, :] = new_theta
              self.sigma_[i, :] = new_sigma
              self.class_count_[i] += N_i
  
          self.sigma_[:, :] += epsilon
  
          # Update if only no priors is provided
          if self.priors is None:
              # 先验通过计数获取
              self.class_prior_ = self.class_count_ / self.class_count_.sum()
  
          return self
  ```

  训练过程非常简单，就是计算三个参数值

- 预测

  ```python
      def predict(self, X):
  
          jll = self._joint_log_likelihood(X)
          return self.classes_[np.argmax(jll, axis=1)]
      
      def _joint_log_likelihood(self, X):
          check_is_fitted(self, "classes_")
  
          X = check_array(X)
          joint_log_likelihood = []
          # 计算X特征属于每个类的非归一化概率
          for i in range(np.size(self.classes_)):
              jointi = np.log(self.class_prior_[i])
              n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))
              n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /
                                   (self.sigma_[i, :]), 1)
              joint_log_likelihood.append(jointi + n_ij)
  
          joint_log_likelihood = np.array(joint_log_likelihood).T
          return joint_log_likelihood
  ```

  `jointi`代表最终公式的第三部分

  `n_ij`两行代码对应似然计算的第一部分和第二部分

  代码中的`sigma`表示$\sigma_{ij}^{2}$，`theta`表示$\mu_{ij}$